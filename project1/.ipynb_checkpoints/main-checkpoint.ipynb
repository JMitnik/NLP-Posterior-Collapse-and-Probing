{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preventing Posterior Collapse in Sentence Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a study is performed to study the phenomenon of Posterior collapse in sentence variational autoencoders, and ways to. A formal introduction to the nature of this phenomenon can be found in the attached paper (or in the Github repository). This notebook provides a high-level overview of the experimental design of this paper. \n",
    "\n",
    "The notebook consists of three stages:\n",
    " - Training\n",
    " - Testing (Inference)\n",
    " - Analysis\n",
    " \n",
    "Furthermore, the notebook relies on various implementations which were abstracted into their own modules. These dependencies can be split into a number of items:\n",
    "- `models/`: The model definitions of the VAE and the RNNLM, defined using PyTorch, and factory functions which create them from a config\n",
    "- `trainers/`: Model-specific trainers where each trainer-module defines three things: The complete trainer loop, what each batch-training looks like, and what results can be extracted from a batch-training. \n",
    "- `tools/`: Model-agnostic construction objects, which are instrumental in the setting up, such as storing results, tokenizers, and data preprocessing\n",
    "- `inference/`: Evaluation-specific logic, such as evaluating individual model-types, or using a trained model to predict next words\n",
    "- `utils`: Simple functional utilities\n",
    "- `config`: Configuration schema which defines the various parameters needed in this pipeline\n",
    "- `metrics`: Custom loss functions such as the ELBO definition, and calculating the perplexity from negative log likelihood\n",
    "\n",
    "The results of this experiment will be stored in `results/`. These results include tabular n-iteration records of training and m-iteration records of validaiton. Also, events for tensorboard are sent here. Saved models are stored in a seperate folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption is that users who run this notebook have the following libraries installed:\n",
    "- Pytorch (1.4.0)\n",
    "- nltk\n",
    "- pandas\n",
    "- numpy\n",
    "- tensorboard (though, roughly similar analysis is shown below using pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch-specific imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import our config and utils\n",
    "import utils\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the approach to parametrize each experimental run using a mutable `config` file. The idea is to define a generic config at the start, and when running multiple experiments within the notebook, we __clone__ the config and mutate the appropriate properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    run_label='Notebook-training', #custom label assigned to training\n",
    "    batch_size=64,\n",
    "    vae_latent_size=16,\n",
    "    embedding_size=256,\n",
    "    rnn_hidden_size=256,\n",
    "    vae_encoder_hidden_size=320,\n",
    "    vae_decoder_hidden_size=320,\n",
    "    vocab_size=10000,\n",
    "    \n",
    "    # Hyperparameters\n",
    "    param_wdropout_k=[0, 0.5, 1], #list of potential parameters for word-dropout\n",
    "    mu_force_beta_param=[0, 2, 3, 5, 10], #list of potential parameters for mu-forcing\n",
    "    freebits_param=[-1, 0.25, 0.5, 1, 2, 8], #list of potential parameters for freebits\n",
    "    \n",
    "    #Booleans we define when automatically running the notebook / script\n",
    "    will_train_rnn=True, \n",
    "    will_train_vae=True,\n",
    "    will_grid_search=False, # Set to true to run through all possible parameters\n",
    "    \n",
    "    nr_epochs=5,\n",
    "    \n",
    "    # Various paths\n",
    "    results_path = 'results',\n",
    "    train_path = '/data/02-21.10way.clean',\n",
    "    valid_path = '/data/22.auto.clean',\n",
    "    test_path  = '/data/23.auto.clean',\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Defining intervals for inside the training loops\n",
    "    validate_every = 300,\n",
    "    print_every = 100,\n",
    "    train_text_gen_every = 3000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we will use the Penn Treebank dataset. We have defined a Custom Dataset which loads this into data-loaders. All models will use PyTorch's default implementation of Adam as optimization algorithm (with a `lr` of `0.001`).\n",
    "\n",
    "Note: for this, we will assume you have stored three fragments of this dataset, and are pointing to them as established in `config.train_path` and `config.valid_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Custom Dataset\n",
      "Data loaders are ready\n",
      "Created a train data loader. Shuffle = True, Batch Size = 64\n",
      "Created a valid data loader. Shuffle = False, Batch Size = 64\n",
      "Length of training data: 623\n",
      "Length of validation data: 27\n"
     ]
    }
   ],
   "source": [
    "from tools.customdata import CustomData\n",
    "cd = CustomData(config)\n",
    "train_loader = cd.get_data_loader(type=\"train\", shuffle=True)\n",
    "valid_loader = cd.get_data_loader(type='valid', shuffle=False)\n",
    "print(f\"Length of training data: {len(train_loader)}\")\n",
    "print(f\"Length of validation data: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline, we define an RNN with hyperparameters as found from 'Bowman et al'. These parameters are fixed, and set in the config (`vocab_size`, `rnn_hidden_size` and `embedding_size`). The creation of rnn is abstracted into its own function, only relying on this config. The RNN will be trained (if the config's `will_train_rnn` allows it), and uses Cross Entropy Loss as criterion for maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.make_rnnlm import make_rnnlm\n",
    "\n",
    "rnn_lm = make_rnnlm(config, trained=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=0,\n",
    "    reduction='sum'\n",
    ")\n",
    "optim = torch.optim.Adam(rnn_lm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training loop, we define a `ResultsWriter` as an abstraction over Tensorboard's SummaryWriter and Pandas, which will store results both in tensorboard, as well as save results in a CSV file, in a subdirectory under results corresponding to `config.run_label` + `rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-80be8f793a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_train_rnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     train_rnn(\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrnn_lm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_lm' is not defined"
     ]
    }
   ],
   "source": [
    "from tools.results_writer import ResultsWriter\n",
    "import trainers.trainer_rnn\n",
    "from trainers.trainer_rnn import train_rnn\n",
    "\n",
    "rnn_results_writer = ResultsWriter(label=f'{config.run_label}--rnn')\n",
    "\n",
    "if config.will_train_rnn:\n",
    "    train_rnn(\n",
    "        rnn_lm,\n",
    "        optim,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        config=config,\n",
    "        nr_epochs=config.nr_epochs,\n",
    "        device=config.device,\n",
    "        results_writer=rnn_results_writer\n",
    ")\n",
    "    \n",
    "# We should not forget to close the writer once we are done\n",
    "rnn_results_writer.tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Sentence VAE's: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we can demonstrate Posterior collapse in various ways. One rather basic wayby simply running the training of a Sentence VAE, and paying attention to it's KL-loss. As the model proceeds through its iterations, the KL gets closer to 0 (thus pushing the posterior to a prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tools.results_writer import ResultsWriter\n",
    "\n",
    "from models.make_vae import make_vae\n",
    "vanilla_params = {\n",
    "    'free_bits_param': 0,\n",
    "    'mu_force_beta_param': 0,\n",
    "    'param_wdropout_k': 1\n",
    "}\n",
    "\n",
    "# Clone config to not mutate original, set the parameters in there\n",
    "vanilla_run_config = deepcopy(config)\n",
    "vanilla_run_config.freebits_param = vanilla_params['free_bits_param']\n",
    "vanilla_run_config.mu_force_beta_param = vanilla_params['mu_force_beta_param']\n",
    "vanilla_run_config.param_wdropout_k = vanilla_params['param_wdropout_k']\n",
    "\n",
    "# Create VAE model, its optimizer\n",
    "vae = make_vae(vanilla_run_config).to(vanilla_run_config.device)\n",
    "vae_optim = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Create a results-writer\n",
    "vae_results_writer = ResultsWriter(label=f'{vanilla_run_config.run_label}--vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, prior to running, we use a 'decoding' tool to transform the words the VAE greedily selected during inference, to transfrom it back into the original source language (English in this case). This decoding-tool uses the same tokenizer as is used in the CustomData to reverse-map the sampled indices back to their original words. The resulting text gets shown in this notebook as well as stored in tensorboard.\n",
    "\n",
    "Note: the predicted sentence generates up until the final padding tag. The cutting off of the padding generation was not finalized, so expect the sentence length between prediction and true to differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 || NLLL: 229.60691833496094 || Perp: 7065.600082313294 || KL Loss: 5.269302845001221 || MuLoss: 0.0 || Total: 234.876220703125\n",
      "VAE is generating sentences on 0: \n",
      "\n",
      "\t The true sentence is: \"and both the trust and manville are seeking to avoid the bad publicity that , in the asbestos era , [UNK] the manville name .\" \n",
      "\n",
      "\t The predicted sentence is: \"ge salmonella realistic complain styles reopen onerous conform repaid eurocom thinking jitters latin harvard 9.6 marketer trusts singled ldp sponsored ig edison looked cool stevens telerate campus contras change murdoch vice 50 fare wachovia eggs republics yamaichi pennsylvania dressed 38 high-grade ship manhattan filing fluor equal fundamentals dominated rubles proof bethlehem placed drove cleanup suggest target suez seize werner worrying ufo estimates catholic bush\" \n",
      "\n",
      "Iteration: 100 || NLLL: 143.69952392578125 || Perp: 277.2543991298171 || KL Loss: 0.031154032796621323 || MuLoss: 0.0 || Total: 143.73068237304688\n",
      "Iteration: 200 || NLLL: 149.41928100585938 || Perp: 238.37863742363393 || KL Loss: 0.05399187654256821 || MuLoss: 0.0 || Total: 149.4732666015625\n",
      "Iteration: 300 || NLLL: 126.97586822509766 || Perp: 172.4047825952844 || KL Loss: 0.041229985654354095 || MuLoss: 0.0 || Total: 127.01709747314453\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 128.8902359008789 || KL loss: 0.04951577799187766 || NLLL 128.84071802209925 || Perp: 156.1488638910592 ||MU loss 0.024483299296763208\n",
      "New Best Validation score of 128.8902359008789!\n",
      "Iteration: 400 || NLLL: 132.13101196289062 || Perp: 140.5065699518287 || KL Loss: 0.04546046629548073 || MuLoss: 0.0 || Total: 132.17648315429688\n",
      "Iteration: 500 || NLLL: 133.70828247070312 || Perp: 133.68653337893912 || KL Loss: 0.04828640818595886 || MuLoss: 0.0 || Total: 133.75656127929688\n",
      "Iteration: 600 || NLLL: 123.45481872558594 || Perp: 130.87185384474927 || KL Loss: 0.05231373757123947 || MuLoss: 0.0 || Total: 123.50712585449219\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 119.87247834382234 || KL loss: 0.05375636575950517 || NLLL 119.81872276023581 || Perp: 111.09673477674141 ||MU loss 0.03480618640228554\n",
      "New Best Validation score of 119.87247834382234!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 119.38028208414714 || KL loss: 0.054076567154239724 || NLLL 119.32620634856048 || Perp: 109.15281551561341 ||MU loss 0.0318757725396642\n",
      "New Best Validation score of 119.38028208414714!\n",
      "Iteration: 700 || NLLL: 117.82463073730469 || Perp: 85.75220894771677 || KL Loss: 0.05242731422185898 || MuLoss: 0.0 || Total: 117.87705993652344\n",
      "Iteration: 800 || NLLL: 116.45869445800781 || Perp: 116.33216032242404 || KL Loss: 0.040066592395305634 || MuLoss: 0.0 || Total: 116.49877166748047\n",
      "Iteration: 900 || NLLL: 109.73881530761719 || Perp: 84.73230662058398 || KL Loss: 0.04675251245498657 || MuLoss: 0.0 || Total: 109.78556823730469\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 115.1502422756619 || KL loss: 0.05080634534910873 || NLLL 115.09943728976779 || Perp: 93.46805359425376 ||MU loss 0.035287002446474855\n",
      "New Best Validation score of 115.1502422756619!\n",
      "Iteration: 1000 || NLLL: 122.86611938476562 || Perp: 101.78153604774701 || KL Loss: 0.049859948456287384 || MuLoss: 0.0 || Total: 122.91598510742188\n",
      "Iteration: 1100 || NLLL: 111.68487548828125 || Perp: 96.84861529096719 || KL Loss: 0.042618632316589355 || MuLoss: 0.0 || Total: 111.72749328613281\n",
      "Iteration: 1200 || NLLL: 122.4159927368164 || Perp: 82.58866257726791 || KL Loss: 0.05427680164575577 || MuLoss: 0.0 || Total: 122.47026062011719\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 111.94747472692418 || KL loss: 0.04522261064913538 || NLLL 111.90225177341037 || Perp: 83.32376780167007 ||MU loss 0.03351041302084923\n",
      "New Best Validation score of 111.94747472692418!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 111.78459958676939 || KL loss: 0.056268237669158866 || NLLL 111.72833096539533 || Perp: 82.79366708976586 ||MU loss 0.03768081597431942\n",
      "New Best Validation score of 111.78459958676939!\n",
      "Iteration: 1300 || NLLL: 106.19058990478516 || Perp: 62.42340215497355 || KL Loss: 0.053266435861587524 || MuLoss: 0.0 || Total: 106.24385070800781\n",
      "Iteration: 1400 || NLLL: 129.02487182617188 || Perp: 79.5226818845613 || KL Loss: 0.0487755686044693 || MuLoss: 0.0 || Total: 129.0736541748047\n",
      "Iteration: 1500 || NLLL: 115.79313659667969 || Perp: 73.59559086966041 || KL Loss: 0.048732560127973557 || MuLoss: 0.0 || Total: 115.84186553955078\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 110.24664447925709 || KL loss: 0.04885076455495976 || NLLL 110.19779233579283 || Perp: 78.66707923157708 ||MU loss 0.03892620321777132\n",
      "New Best Validation score of 110.24664447925709!\n",
      "Iteration: 1600 || NLLL: 115.55865478515625 || Perp: 72.41608436271432 || KL Loss: 0.04825396090745926 || MuLoss: 0.0 || Total: 115.60690307617188\n",
      "Iteration: 1700 || NLLL: 122.54045104980469 || Perp: 73.67510465483683 || KL Loss: 0.04516158998012543 || MuLoss: 0.0 || Total: 122.58560943603516\n",
      "Iteration: 1800 || NLLL: 111.67587280273438 || Perp: 61.089814092551904 || KL Loss: 0.049943096935749054 || MuLoss: 0.0 || Total: 111.72582244873047\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 108.30203218813296 || KL loss: 0.05086839350837248 || NLLL 108.25116418909144 || Perp: 73.28106112762367 ||MU loss 0.04328109089423109\n",
      "New Best Validation score of 108.30203218813296!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 108.32790798611111 || KL loss: 0.05435661502458431 || NLLL 108.27355066935222 || Perp: 73.2938523141387 ||MU loss 0.04259588003710464\n",
      "Iteration: 1900 || NLLL: 101.3653564453125 || Perp: 47.10433761685876 || KL Loss: 0.05194103717803955 || MuLoss: 0.0 || Total: 101.41729736328125\n",
      "Iteration: 2000 || NLLL: 97.87808227539062 || Perp: 49.78910441226201 || KL Loss: 0.05351771414279938 || MuLoss: 0.0 || Total: 97.93159484863281\n",
      "Iteration: 2100 || NLLL: 100.15262603759766 || Perp: 49.230607901611634 || KL Loss: 0.05401288717985153 || MuLoss: 0.0 || Total: 100.20663452148438\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.7609959355107 || KL loss: 0.05275990772578451 || NLLL 107.70823542277019 || Perp: 72.10849887404291 ||MU loss 0.0423025988318302\n",
      "New Best Validation score of 107.7609959355107!\n",
      "Iteration: 2200 || NLLL: 93.48957824707031 || Perp: 47.23996923525349 || KL Loss: 0.0544859804213047 || MuLoss: 0.0 || Total: 93.5440673828125\n",
      "Iteration: 2300 || NLLL: 99.28277587890625 || Perp: 52.01609910361167 || KL Loss: 0.05076880007982254 || MuLoss: 0.0 || Total: 99.33354949951172\n",
      "Iteration: 2400 || NLLL: 94.16181945800781 || Perp: 44.263538805351615 || KL Loss: 0.04688145965337753 || MuLoss: 0.0 || Total: 94.20870208740234\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.16672318070023 || KL loss: 0.05541670460391928 || NLLL 107.11130679095233 || Perp: 70.74060530618964 ||MU loss 0.04417462577974355\n",
      "New Best Validation score of 107.16672318070023!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 106.99978906136973 || KL loss: 0.05232108812089319 || NLLL 106.94746836909542 || Perp: 70.3136161074021 ||MU loss 0.04363858768785441\n",
      "New Best Validation score of 106.99978906136973!\n",
      "Iteration: 2500 || NLLL: 102.04156494140625 || Perp: 44.17177177778486 || KL Loss: 0.04964388161897659 || MuLoss: 0.0 || Total: 102.0912094116211\n",
      "Iteration: 2600 || NLLL: 104.97008514404297 || Perp: 47.30120870317333 || KL Loss: 0.050282709300518036 || MuLoss: 0.0 || Total: 105.0203628540039\n",
      "Iteration: 2700 || NLLL: 92.81745147705078 || Perp: 35.666001311891975 || KL Loss: 0.0337459035217762 || MuLoss: 0.0 || Total: 92.8511962890625\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.20723653722692 || KL loss: 0.04632292315363884 || NLLL 107.16091099491825 || Perp: 71.19331767752948 ||MU loss 0.038218167437999336\n",
      "Iteration: 2800 || NLLL: 97.34944152832031 || Perp: 44.4504599400736 || KL Loss: 0.04904837906360626 || MuLoss: 0.0 || Total: 97.39849853515625\n",
      "Iteration: 2900 || NLLL: 98.19682312011719 || Perp: 46.70214140673442 || KL Loss: 0.03975321352481842 || MuLoss: 0.0 || Total: 98.236572265625\n",
      "Iteration: 3000 || NLLL: 101.4611587524414 || Perp: 42.02859084056565 || KL Loss: 0.04677549749612808 || MuLoss: 0.0 || Total: 101.5079345703125\n",
      "VAE is generating sentences on 3000: \n",
      "\n",
      "\t The true sentence is: \"prime rate : 10 1\\/2 % .\" \n",
      "\n",
      "\t The predicted sentence is: \"the minister was 10 1\\/2 point . and ; ; 9 . . [UNK] . ; . . and 15 , . 9 ; . , , . ltd . , , vs. , . . and . 1 . . and . ; in . ; . . ; & . . [UNK] . ; , . and ;\" \n",
      "\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 106.83383362381547 || KL loss: 0.046490039538454125 || NLLL 106.7873439082393 || Perp: 70.36851800509183 ||MU loss 0.038924213981738794\n",
      "New Best Validation score of 106.83383362381547!\n",
      "Iteration: 3100 || NLLL: 93.77741241455078 || Perp: 41.30008147564754 || KL Loss: 0.048066746443510056 || MuLoss: 0.0 || Total: 93.82547760009766\n",
      "Done training the VAE\n"
     ]
    }
   ],
   "source": [
    "from trainers.trainer_vae import train_vae\n",
    "\n",
    "# Decoding tool (different from a neural network decoder: samples from categorical distribution, \n",
    "# and reverses back to original word using tokenizer and its vocab.)\n",
    "sentence_decoder = utils.make_sentence_decoder(cd.tokenizer, 1)\n",
    "\n",
    "if vanilla_run_config.will_train_vae:\n",
    "    train_vae(\n",
    "        vae,\n",
    "        vae_optim,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        nr_epochs=vanilla_run_config.nr_epochs,\n",
    "        device=vanilla_run_config.device,\n",
    "        results_writer=vae_results_writer,\n",
    "        config=vanilla_run_config,\n",
    "        decoder=sentence_decoder,\n",
    "    )\n",
    "\n",
    "vae_results_writer.tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countering the Posterior collapse: Training three methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one might notice, the KL is incredibly low. This means that the generated Posterior distribution has lost all latent information, and has reverted to a standard Gaussian prior. Previously, we had done a grid search over all possible values. However, to keep things brief, we define the four best setups we had found for our three main hyperparameters. More details about these can be found in the paper.\n",
    "\n",
    "Note: Word dropout was found to improve nothing, hence it has been set to 1 (meaning not apply word dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "# Subset of parameter-grid, with best hyperparameter. \n",
    "# These can be adjusted and played around with. \n",
    "param_grid = [\n",
    "    # Very best setup (validation perplexity of 103)\n",
    "    {\n",
    "        'free_bits_param': 2,\n",
    "        'mu_force_beta_param': 2,\n",
    "        'param_wdropout_k': 1\n",
    "    },\n",
    "    # Best only free-bits (validation perplexity of 102.3)\n",
    "    {\n",
    "        'free_bits_param': 2,\n",
    "        'mu_force_beta_param': 0,\n",
    "        'param_wdropout_k': 1\n",
    "    },\n",
    "    \n",
    "    # Best only mu-force (validation perplexity of 111)\n",
    "    {\n",
    "        'free_bits_param': 0,\n",
    "        'mu_force_beta_param': 5,\n",
    "        'param_wdropout_k': 1\n",
    "    },\n",
    "]\n",
    "\n",
    "# However, we could do grid search as well.\n",
    "\n",
    "# Uncomment if you want to do grid search\n",
    "# config.will_grid_search = True\n",
    "if config.will_grid_search:\n",
    "    potential_params = OrderedDict({\n",
    "        'free_bits_param': np.hstack([config.freebits_param]),\n",
    "        'param_wdropout_k': np.hstack([config.param_wdropout_k]),\n",
    "        'mu_force_beta_param': np.hstack([config.mu_force_beta_param]),\n",
    "    })\n",
    "    \n",
    "    param_grid = utils.make_param_grid(potential_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run training for all these parameters, using a similar code approach as before. This will result in saved models (best models get saved in `config.results_path/saved_models`, and saved-results in our `config.results_path/{run}` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training params: free_bits_param:2-mu_force_beta_param:2-param_wdropout_k:1\n",
      "Iteration: 0 || NLLL: 238.51205444335938 || Perp: 7224.250485393984 || KL Loss: 32.0 || MuLoss: 1.9688620567321777 || Total: 272.48089599609375\n",
      "VAE is generating sentences on 0: \n",
      "\n",
      "\t The true sentence is: \"british life insurer london & general , which firmed 2 pence to [UNK] pence -lrb- $ [UNK] -rrb- , and composite insurer royal insurance , which finished 13 lower at 475 , were featured in the talk .\" \n",
      "\n",
      "\t The predicted sentence is: \"formula severely dominate scientist authors accuse pricings municipals comedy offensive guinness jacobson teagan distributes preclude prosperity ousted memorandum proceed defaulted eroded relocation ecological shown leaks color command intellectual criminal committees oliver phone pork-barrel institution democratic studies creation reinforcement desirable windsor pitney bourbon arise enthusiasm usage suppliers traffic tiger snow corn delta meets n.j pushing turbines ingersoll cheney shape homes 0.5 1975\" \n",
      "\n",
      "Iteration: 100 || NLLL: 138.22518920898438 || Perp: 293.44884931813186 || KL Loss: 32.0323486328125 || MuLoss: 0.0 || Total: 170.2575225830078\n",
      "Iteration: 200 || NLLL: 133.3192596435547 || Perp: 198.30194328614266 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 165.3192596435547\n",
      "Iteration: 300 || NLLL: 123.45647430419922 || Perp: 152.84390290801804 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 155.45648193359375\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 159.28072668004918 || KL loss: 32.043406027334704 || NLLL 127.23732107656973 || Perp: 147.00587538608383 ||MU loss 20.260362130624276\n",
      "New Best Validation score of 159.28072668004918!\n",
      "Iteration: 400 || NLLL: 129.25025939941406 || Perp: 140.80533733535447 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 161.25025939941406\n",
      "Iteration: 500 || NLLL: 117.14952087402344 || Perp: 120.39513539886617 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 149.14952087402344\n",
      "Iteration: 600 || NLLL: 127.57485961914062 || Perp: 110.6012533286545 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 159.57485961914062\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 149.1364791304977 || KL loss: 32.10964005081742 || NLLL 117.02683625397859 || Perp: 100.28921642172243 ||MU loss 39.69179054542824\n",
      "New Best Validation score of 149.1364791304977!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 148.75405544704861 || KL loss: 32.025318993462456 || NLLL 116.72873885543258 || Perp: 99.17736985306284 ||MU loss 38.43334070841471\n",
      "New Best Validation score of 148.75405544704861!\n",
      "Iteration: 700 || NLLL: 117.26918029785156 || Perp: 98.25315827398137 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 149.26918029785156\n",
      "Iteration: 800 || NLLL: 107.50912475585938 || Perp: 90.82201331024807 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 139.50912475585938\n",
      "Iteration: 900 || NLLL: 123.0956802368164 || Perp: 87.90059327205756 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 155.09567260742188\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 143.70693348072194 || KL loss: 32.06751646818938 || NLLL 111.63941559968171 || Perp: 81.92340984571868 ||MU loss 51.288208149097585\n",
      "New Best Validation score of 143.70693348072194!\n",
      "Iteration: 1000 || NLLL: 127.33070373535156 || Perp: 80.88924583059838 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 159.33070373535156\n",
      "Iteration: 1100 || NLLL: 116.27267456054688 || Perp: 84.55135293798344 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 148.27268981933594\n",
      "Iteration: 1200 || NLLL: 109.84091186523438 || Perp: 60.715699939502606 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 141.84091186523438\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 140.1512318363896 || KL loss: 32.056756761338974 || NLLL 108.0944762053313 || Perp: 71.98565212261384 ||MU loss 50.36262243765372\n",
      "New Best Validation score of 140.1512318363896!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 139.87014318395543 || KL loss: 32.30535549587674 || NLLL 107.56478599265769 || Perp: 70.55448549099464 ||MU loss 52.96805756180375\n",
      "New Best Validation score of 139.87014318395543!\n",
      "Iteration: 1300 || NLLL: 98.33612060546875 || Perp: 58.145669215113266 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 130.33612060546875\n",
      "Iteration: 1400 || NLLL: 94.48334503173828 || Perp: 52.457809714069256 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 126.48334503173828\n",
      "Iteration: 1500 || NLLL: 102.914306640625 || Perp: 53.76286328105561 || KL Loss: 32.0531005859375 || MuLoss: 0.0 || Total: 134.9674072265625\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 137.6324621129919 || KL loss: 32.045023600260414 || NLLL 105.58743964301215 || Perp: 65.78288847531937 ||MU loss 55.90977435641818\n",
      "New Best Validation score of 137.6324621129919!\n",
      "Iteration: 1600 || NLLL: 100.37651062011719 || Perp: 54.46569486455429 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 132.3765106201172\n",
      "Iteration: 1700 || NLLL: 105.69271850585938 || Perp: 58.27100089855776 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 137.69271850585938\n",
      "Iteration: 1800 || NLLL: 100.7523193359375 || Perp: 47.52076289161097 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 132.7523193359375\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 135.87615288628473 || KL loss: 32.25125418768989 || NLLL 103.62489770959925 || Perp: 61.23466146235002 ||MU loss 56.498418454770686\n",
      "New Best Validation score of 135.87615288628473!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 135.67075234872323 || KL loss: 32.27038093849465 || NLLL 103.40037564878111 || Perp: 60.72909365712944 ||MU loss 56.97828250461154\n",
      "New Best Validation score of 135.67075234872323!\n",
      "Iteration: 1900 || NLLL: 92.0900650024414 || Perp: 39.516123209875595 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 124.0900650024414\n",
      "Iteration: 2000 || NLLL: 109.04766845703125 || Perp: 52.034001746014766 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 141.04766845703125\n",
      "Iteration: 2100 || NLLL: 102.93659973144531 || Perp: 44.08738923749948 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 134.9365997314453\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 134.8605948554145 || KL loss: 32.24837296097367 || NLLL 102.61222245958116 || Perp: 59.12389806953833 ||MU loss 58.845499391909\n",
      "New Best Validation score of 134.8605948554145!\n",
      "Iteration: 2200 || NLLL: 98.31453704833984 || Perp: 40.94260919534517 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 130.31454467773438\n",
      "Iteration: 2300 || NLLL: 89.102294921875 || Perp: 35.072650240582654 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 121.102294921875\n",
      "Iteration: 2400 || NLLL: 100.91325378417969 || Perp: 48.827277816824925 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 132.9132537841797\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 133.70496340151186 || KL loss: 32.39536228886357 || NLLL 101.30960167778863 || Perp: 56.261412380977404 ||MU loss 59.60335795084635\n",
      "New Best Validation score of 133.70496340151186!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 133.69638711434823 || KL loss: 32.13613255818685 || NLLL 101.56025526258681 || Perp: 56.9150206868141 ||MU loss 59.28975437305592\n",
      "New Best Validation score of 133.69638711434823!\n",
      "Iteration: 2500 || NLLL: 89.28821563720703 || Perp: 30.941538836192624 || KL Loss: 32.0157470703125 || MuLoss: 0.0 || Total: 121.30397033691406\n",
      "Iteration: 2600 || NLLL: 87.59539794921875 || Perp: 32.45620567052443 || KL Loss: 32.16704559326172 || MuLoss: 0.0 || Total: 119.762451171875\n",
      "Iteration: 2700 || NLLL: 77.44114685058594 || Perp: 28.340363015550626 || KL Loss: 32.233253479003906 || MuLoss: 0.0 || Total: 109.67440795898438\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 133.57044106942635 || KL loss: 32.444726873327184 || NLLL 101.12571334838867 || Perp: 56.16085593576967 ||MU loss 61.748869295473455\n",
      "New Best Validation score of 133.57044106942635!\n",
      "Iteration: 2800 || NLLL: 92.31044006347656 || Perp: 34.753113586381794 || KL Loss: 32.16368103027344 || MuLoss: 0.0 || Total: 124.47412872314453\n",
      "Iteration: 2900 || NLLL: 86.11538696289062 || Perp: 32.655453454100034 || KL Loss: 32.308719635009766 || MuLoss: 0.0 || Total: 118.42410278320312\n",
      "Iteration: 3000 || NLLL: 87.28720092773438 || Perp: 30.7911564680248 || KL Loss: 32.02888870239258 || MuLoss: 0.0 || Total: 119.31608581542969\n",
      "VAE is generating sentences on 3000: \n",
      "\n",
      "\t The true sentence is: \"the group is divided primarily between software , semiconductors and computers .\" \n",
      "\n",
      "\t The predicted sentence is: \"the high is located yet for october periods and , local on , here for . . . that . . [UNK] . . and . . . . to . . . . . . . . . . . . . . .\" \n",
      "\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 132.76796326813874 || KL loss: 32.33652538723416 || NLLL 100.4314394350405 || Perp: 54.77986710819749 ||MU loss 61.63963218971535\n",
      "New Best Validation score of 132.76796326813874!\n",
      "Iteration: 3100 || NLLL: 85.87911224365234 || Perp: 30.189033373514558 || KL Loss: 32.13294982910156 || MuLoss: 0.0 || Total: 118.01205444335938\n",
      "Done training the VAE\n",
      "Finished training for free_bits_param:2-mu_force_beta_param:2-param_wdropout_k:1!!!\n",
      "Training params: free_bits_param:2-mu_force_beta_param:0-param_wdropout_k:1\n",
      "Iteration: 0 || NLLL: 220.33700561523438 || Perp: 6951.099875741634 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 252.3369903564453\n",
      "VAE is generating sentences on 0: \n",
      "\n",
      "\t The true sentence is: \"the company did n't provide an explanation for the softer pretax profit performance and officials could n't be reached for comment .\" \n",
      "\n",
      "\t The predicted sentence is: \"boosted oust reiterated boveri lebanon tax-loss mostly someone deter managed specialty double-a canceled chicken accumulated deficits d. allegedly clothing safer model attribute weight rights 6.4 conn fresenius pursuit datapoint lang unesco constituency savings upgraded silly false affidavits techniques marking duck responsibility thousands viewed constituency actively chose intelligence lebanon bargain\" \n",
      "\n",
      "Iteration: 100 || NLLL: 143.34527587890625 || Perp: 295.1841181332907 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 175.34527587890625\n",
      "Iteration: 200 || NLLL: 140.5025634765625 || Perp: 202.62939188348773 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 172.5025634765625\n",
      "Iteration: 300 || NLLL: 143.91336059570312 || Perp: 190.20882577517696 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 175.91336059570312\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 159.32276125307436 || KL loss: 32.039915296766495 || NLLL 127.28284454345703 || Perp: 146.88738344595563 ||MU loss 14.760767548172563\n",
      "New Best Validation score of 159.32276125307436!\n",
      "Iteration: 400 || NLLL: 119.2184066772461 || Perp: 146.00727534011997 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 151.21841430664062\n",
      "Iteration: 500 || NLLL: 119.14776611328125 || Perp: 120.284520944941 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 151.14776611328125\n",
      "Iteration: 600 || NLLL: 125.11056518554688 || Perp: 123.68467238470328 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 157.11056518554688\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 149.6399541784216 || KL loss: 32.08607129697447 || NLLL 117.55387991446035 || Perp: 101.9495993542923 ||MU loss 41.09955201325593\n",
      "New Best Validation score of 149.6399541784216!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 149.18566781503182 || KL loss: 32.27146261709708 || NLLL 116.91420604564526 || Perp: 99.7104812237576 ||MU loss 44.321710939760564\n",
      "New Best Validation score of 149.18566781503182!\n",
      "Iteration: 700 || NLLL: 117.49829864501953 || Perp: 106.15973134458922 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 149.498291015625\n",
      "Iteration: 800 || NLLL: 118.07247924804688 || Perp: 86.55894805705518 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 150.07247924804688\n",
      "Iteration: 900 || NLLL: 117.81871795654297 || Perp: 106.58865267240847 || KL Loss: 32.25266647338867 || MuLoss: 0.0 || Total: 150.07138061523438\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 144.43298565899883 || KL loss: 32.130483132821546 || NLLL 112.30250295003255 || Perp: 84.18306407921366 ||MU loss 57.66464346426505\n",
      "New Best Validation score of 144.43298565899883!\n",
      "Iteration: 1000 || NLLL: 119.66563415527344 || Perp: 86.75204174444585 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 151.66561889648438\n",
      "Iteration: 1100 || NLLL: 108.29408264160156 || Perp: 75.05856803548633 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 140.29408264160156\n",
      "Iteration: 1200 || NLLL: 97.34205627441406 || Perp: 62.94209398425107 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 129.34205627441406\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 140.43409474690756 || KL loss: 32.13265736897787 || NLLL 108.30143836692527 || Perp: 72.46017021459882 ||MU loss 67.17155385900426\n",
      "New Best Validation score of 140.43409474690756!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 140.19135849564165 || KL loss: 32.10925434253834 || NLLL 108.08210160997179 || Perp: 71.92123251133495 ||MU loss 67.61091726797598\n",
      "New Best Validation score of 140.19135849564165!\n",
      "Iteration: 1300 || NLLL: 104.87762451171875 || Perp: 67.4169075197295 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 136.87762451171875\n",
      "Iteration: 1400 || NLLL: 105.58329010009766 || Perp: 49.58631770934225 || KL Loss: 32.02925491333008 || MuLoss: 0.0 || Total: 137.612548828125\n",
      "Iteration: 1500 || NLLL: 96.63385009765625 || Perp: 52.42531076864092 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 128.63385009765625\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 138.25280959517866 || KL loss: 32.03904554578993 || NLLL 106.21376376681857 || Perp: 67.37311022046107 ||MU loss 66.63989271941008\n",
      "New Best Validation score of 138.25280959517866!\n",
      "Iteration: 1600 || NLLL: 106.28831481933594 || Perp: 64.92829626748852 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 138.288330078125\n",
      "Iteration: 1700 || NLLL: 95.90511322021484 || Perp: 47.02335745917039 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 127.90512084960938\n",
      "Iteration: 1800 || NLLL: 102.69285583496094 || Perp: 47.4308803960242 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 134.69285583496094\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 136.34515550401477 || KL loss: 32.17785022876881 || NLLL 104.16730810094762 || Perp: 62.44185405815392 ||MU loss 69.2578221073857\n",
      "New Best Validation score of 136.34515550401477!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 136.31379643192997 || KL loss: 32.412507940221715 || NLLL 103.90128764399776 || Perp: 61.82183869346704 ||MU loss 69.27092771176939\n",
      "New Best Validation score of 136.31379643192997!\n",
      "Iteration: 1900 || NLLL: 93.7556381225586 || Perp: 39.426579515950024 || KL Loss: 32.155731201171875 || MuLoss: 0.0 || Total: 125.911376953125\n",
      "Iteration: 2000 || NLLL: 92.11940002441406 || Perp: 37.47324795210426 || KL Loss: 32.002235412597656 || MuLoss: 0.0 || Total: 124.12164306640625\n",
      "Iteration: 2100 || NLLL: 92.21185302734375 || Perp: 38.989368607341355 || KL Loss: 32.11844253540039 || MuLoss: 0.0 || Total: 124.3302993774414\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 135.51894180862993 || KL loss: 32.14040968153212 || NLLL 103.37853297480831 || Perp: 60.87547087334451 ||MU loss 69.29711009837963\n",
      "New Best Validation score of 135.51894180862993!\n",
      "Iteration: 2200 || NLLL: 109.15355682373047 || Perp: 44.92152207678993 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 141.15354919433594\n",
      "Iteration: 2300 || NLLL: 106.09596252441406 || Perp: 47.2685032439938 || KL Loss: 32.035675048828125 || MuLoss: 0.0 || Total: 138.1316375732422\n",
      "Iteration: 2400 || NLLL: 98.61468505859375 || Perp: 52.16822464729679 || KL Loss: 32.035579681396484 || MuLoss: 0.0 || Total: 130.6502685546875\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 134.2736254091616 || KL loss: 32.226950327555336 || NLLL 102.0466756467466 || Perp: 57.95145201340087 ||MU loss 69.73251568829572\n",
      "New Best Validation score of 134.2736254091616!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 134.29599055537471 || KL loss: 32.11463730423539 || NLLL 102.18135296856916 || Perp: 58.28094486328154 ||MU loss 68.40875131112558\n",
      "Iteration: 2500 || NLLL: 85.49484252929688 || Perp: 29.29960474056036 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 117.49484252929688\n",
      "Iteration: 2600 || NLLL: 103.65547943115234 || Perp: 38.283838969977786 || KL Loss: 32.44145202636719 || MuLoss: 0.0 || Total: 136.096923828125\n",
      "Iteration: 2700 || NLLL: 87.83457946777344 || Perp: 37.06723203289946 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 119.83457946777344\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 134.27866787380643 || KL loss: 32.277335979320384 || NLLL 102.00133260091145 || Perp: 58.20745834484605 ||MU loss 70.63553308557582\n",
      "Iteration: 2800 || NLLL: 98.25898742675781 || Perp: 43.98020243633246 || KL Loss: 32.107330322265625 || MuLoss: 0.0 || Total: 130.36630249023438\n",
      "Iteration: 2900 || NLLL: 95.36013793945312 || Perp: 33.63546856283754 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 127.36013793945312\n",
      "Iteration: 3000 || NLLL: 86.88497161865234 || Perp: 31.962779238025906 || KL Loss: 32.0604133605957 || MuLoss: 0.0 || Total: 118.94538116455078\n",
      "VAE is generating sentences on 3000: \n",
      "\n",
      "\t The true sentence is: \"mr. mitterrand proposed that a conference be [UNK] next fall to write a new treaty for the ec allowing a european central bank , and that the treaty be [UNK] by 1992 .\" \n",
      "\n",
      "\t The predicted sentence is: \"mr. [UNK] has that [UNK] [UNK] is kidder upon year of adopt them single system [UNK] the removal pool her plan panamanian bank , found policy much state supreme intend to state . covering to that a by by in a as to to at before\" \n",
      "\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 133.5727731210214 || KL loss: 32.11999836674443 || NLLL 101.45277602584274 || Perp: 57.02798019690044 ||MU loss 71.39035147207754\n",
      "New Best Validation score of 133.5727731210214!\n",
      "Iteration: 3100 || NLLL: 83.34271240234375 || Perp: 34.206180820211905 || KL Loss: 32.0 || MuLoss: 0.0 || Total: 115.34271240234375\n",
      "Done training the VAE\n",
      "Finished training for free_bits_param:2-mu_force_beta_param:0-param_wdropout_k:1!!!\n",
      "Training params: free_bits_param:0-mu_force_beta_param:5-param_wdropout_k:1\n",
      "Iteration: 0 || NLLL: 207.21481323242188 || Perp: 6913.044164796037 || KL Loss: 5.335829257965088 || MuLoss: 4.963152885437012 || Total: 217.51377868652344\n",
      "VAE is generating sentences on 0: \n",
      "\n",
      "\t The true sentence is: \"the white house has decided to push for changes in pesticide law that are designed to speed the removal of harmful chemicals from the nation 's food supply .\" \n",
      "\n",
      "\t The predicted sentence is: \"interested broke painewebber numbers could hepatitis zero-coupon kick damages offers prisoner politically dorfman blocks seven-day solutions resumed producer process potentially pays homes dialogue published inventories double-digit received resignation diverted hurry disk-drive certified proves calgary-based appear roh aer erode lives snack-food pouring firms mitterrand genuine toilet sounds autumn supporting shift ignoring ellis wind 35,000 bobby pressing barometer complains sets fleets\" \n",
      "\n",
      "Iteration: 100 || NLLL: 158.1298828125 || Perp: 287.18598394321106 || KL Loss: 0.9197443723678589 || MuLoss: 0.0 || Total: 159.04962158203125\n",
      "Iteration: 200 || NLLL: 152.84698486328125 || Perp: 239.16641147587828 || KL Loss: 0.6162034273147583 || MuLoss: 0.0 || Total: 153.4631805419922\n",
      "Iteration: 300 || NLLL: 129.14492797851562 || Perp: 186.3968815889121 || KL Loss: 0.5450628995895386 || MuLoss: 0.0 || Total: 129.6899871826172\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 128.58232484040437 || KL loss: 0.7158266018938135 || NLLL 127.86649746365018 || Perp: 150.53789366356668 ||MU loss 10.680222511291504\n",
      "New Best Validation score of 128.58232484040437!\n",
      "Iteration: 400 || NLLL: 147.48316955566406 || Perp: 156.06194270971966 || KL Loss: 0.9124237298965454 || MuLoss: 0.0 || Total: 148.3955841064453\n",
      "Iteration: 500 || NLLL: 109.94465637207031 || Perp: 116.08974070657443 || KL Loss: 0.6604995727539062 || MuLoss: 0.0 || Total: 110.60515594482422\n",
      "Iteration: 600 || NLLL: 119.19723510742188 || Perp: 101.27206022929388 || KL Loss: 0.8387138247489929 || MuLoss: 0.0 || Total: 120.03594970703125\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 119.48480478922527 || KL loss: 0.6205999387635125 || NLLL 118.86130467167607 || Perp: 107.6577011934731 ||MU loss 7.447569794125027\n",
      "New Best Validation score of 119.48480478922527!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 119.0697775946723 || KL loss: 0.6731365124384562 || NLLL 118.39664261429398 || Perp: 105.73642809986097 ||MU loss 10.48935192602652\n",
      "New Best Validation score of 119.0697775946723!\n",
      "Iteration: 700 || NLLL: 128.0282440185547 || Perp: 99.03937042611221 || KL Loss: 0.4513639509677887 || MuLoss: 0.0 || Total: 128.4796142578125\n",
      "Iteration: 800 || NLLL: 109.93650817871094 || Perp: 88.11436714572272 || KL Loss: 0.8286301493644714 || MuLoss: 0.0 || Total: 110.76513671875\n",
      "Iteration: 900 || NLLL: 115.81295776367188 || Perp: 94.90188310354749 || KL Loss: 0.6963793039321899 || MuLoss: 0.0 || Total: 116.50933837890625\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 114.89456402813947 || KL loss: 0.716382584086171 || NLLL 114.17818168357566 || Perp: 90.7768971772518 ||MU loss 14.247650464375814\n",
      "New Best Validation score of 114.89456402813947!\n",
      "Iteration: 1000 || NLLL: 104.48265838623047 || Perp: 75.79893412590387 || KL Loss: 0.37314024567604065 || MuLoss: 0.0 || Total: 104.85580444335938\n",
      "Iteration: 1100 || NLLL: 120.10389709472656 || Perp: 80.42756546746652 || KL Loss: 0.8386313915252686 || MuLoss: 0.0 || Total: 120.94252014160156\n",
      "Iteration: 1200 || NLLL: 119.03982543945312 || Perp: 85.41221299662617 || KL Loss: 0.5598528385162354 || MuLoss: 0.0 || Total: 119.59967803955078\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 111.70454604537399 || KL loss: 0.6088409523169199 || NLLL 111.09570340757017 || Perp: 80.99994171456534 ||MU loss 8.143448052582917\n",
      "New Best Validation score of 111.70454604537399!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 111.46058372214988 || KL loss: 0.6656258227648558 || NLLL 110.79495691370082 || Perp: 80.06212717422062 ||MU loss 11.852343930138481\n",
      "New Best Validation score of 111.46058372214988!\n",
      "Iteration: 1300 || NLLL: 111.14440155029297 || Perp: 56.78768755955929 || KL Loss: 0.6945199966430664 || MuLoss: 0.0 || Total: 111.83892059326172\n",
      "Iteration: 1400 || NLLL: 107.4637222290039 || Perp: 68.88354793961204 || KL Loss: 0.30330339074134827 || MuLoss: 0.0 || Total: 107.76702880859375\n",
      "Iteration: 1500 || NLLL: 102.51791381835938 || Perp: 55.993927197211384 || KL Loss: 0.9109647274017334 || MuLoss: 0.0 || Total: 103.42887878417969\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 109.95884181835034 || KL loss: 0.6143311074486485 || NLLL 109.34451039632161 || Perp: 76.19932880867971 ||MU loss 10.453813464553267\n",
      "New Best Validation score of 109.95884181835034!\n",
      "Iteration: 1600 || NLLL: 114.95116424560547 || Perp: 80.18298935596948 || KL Loss: 0.4127222001552582 || MuLoss: 0.0 || Total: 115.36389923095703\n",
      "Iteration: 1700 || NLLL: 98.92680358886719 || Perp: 58.80374557961912 || KL Loss: 0.6785923838615417 || MuLoss: 0.0 || Total: 99.60540008544922\n",
      "Iteration: 1800 || NLLL: 107.02359008789062 || Perp: 61.94275810334517 || KL Loss: 0.6391726732254028 || MuLoss: 0.0 || Total: 107.66275787353516\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 108.66708727236147 || KL loss: 0.6231526953202707 || NLLL 108.04393598768446 || Perp: 72.85394847467175 ||MU loss 11.814965407053629\n",
      "New Best Validation score of 108.66708727236147!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 108.47130754258897 || KL loss: 0.6248197864603113 || NLLL 107.84648867006655 || Perp: 72.32626953058907 ||MU loss 11.184315080995914\n",
      "New Best Validation score of 108.47130754258897!\n",
      "Iteration: 1900 || NLLL: 98.69692993164062 || Perp: 45.98174890965193 || KL Loss: 0.5390086770057678 || MuLoss: 0.0 || Total: 99.23593139648438\n",
      "Iteration: 2000 || NLLL: 96.0958251953125 || Perp: 45.0661663170059 || KL Loss: 0.5304620862007141 || MuLoss: 0.0 || Total: 96.62628173828125\n",
      "Iteration: 2100 || NLLL: 112.05259704589844 || Perp: 52.22308619028555 || KL Loss: 0.7164374589920044 || MuLoss: 0.0 || Total: 112.76903533935547\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.8732664320204 || KL loss: 0.5667064256138272 || NLLL 107.29913471363209 || Perp: 71.0291382686076 ||MU loss 7.589619813142\n",
      "New Best Validation score of 107.8732664320204!\n",
      "Iteration: 2200 || NLLL: 101.73020935058594 || Perp: 58.06673121415187 || KL Loss: 0.7987281084060669 || MuLoss: 0.0 || Total: 102.5289306640625\n",
      "Iteration: 2300 || NLLL: 108.35037231445312 || Perp: 56.35335405474626 || KL Loss: 0.6604770421981812 || MuLoss: 0.0 || Total: 109.01084899902344\n",
      "Iteration: 2400 || NLLL: 101.36982727050781 || Perp: 49.11473201322986 || KL Loss: 0.7116817235946655 || MuLoss: 0.0 || Total: 102.08150482177734\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.12502274689851 || KL loss: 0.637526007714095 || NLLL 106.48749570493345 || Perp: 69.16987546423674 ||MU loss 12.189319822523329\n",
      "New Best Validation score of 107.12502274689851!\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.15504653365524 || KL loss: 0.6152482739201298 || NLLL 106.53979788886176 || Perp: 69.3684366023701 ||MU loss 10.629031198996085\n",
      "Iteration: 2500 || NLLL: 83.73371887207031 || Perp: 36.38672081277598 || KL Loss: 0.548957109451294 || MuLoss: 0.0 || Total: 84.28266906738281\n",
      "Iteration: 2600 || NLLL: 90.87297058105469 || Perp: 43.13424494674566 || KL Loss: 0.7075867652893066 || MuLoss: 0.0 || Total: 91.58055114746094\n",
      "Iteration: 2700 || NLLL: 97.9100341796875 || Perp: 39.20326612885232 || KL Loss: 0.326191782951355 || MuLoss: 0.0 || Total: 98.2362289428711\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 107.25791761610243 || KL loss: 0.6131455876209118 || NLLL 106.64477426034433 || Perp: 69.97805468382204 ||MU loss 11.313430998060438\n",
      "Iteration: 2800 || NLLL: 95.56910705566406 || Perp: 39.917429215966756 || KL Loss: 0.6757712364196777 || MuLoss: 0.0 || Total: 96.24488067626953\n",
      "Iteration: 2900 || NLLL: 96.71781158447266 || Perp: 36.250472010956024 || KL Loss: 0.3915891945362091 || MuLoss: 0.0 || Total: 97.10940551757812\n",
      "Iteration: 3000 || NLLL: 90.962158203125 || Perp: 40.67652957373437 || KL Loss: 0.6995880603790283 || MuLoss: 0.0 || Total: 91.6617431640625\n",
      "VAE is generating sentences on 3000: \n",
      "\n",
      "\t The true sentence is: \"mr. thornburgh will be free to [UNK] the strike forces after congress approves a $ [UNK] million [UNK] for federal law-enforcement and [UNK] agencies , according to david runkel , a justice department spokesman .\" \n",
      "\n",
      "\t The predicted sentence is: \"as [UNK] as report [UNK] to a peace existence president today failing 's his kong 2,000 commitment on involving the projects to state 's to who to the union , great [UNK] department with said sen. . in initially . for . . in with . [UNK] , to from in that . for from for in .\" \n",
      "\n",
      "Validating model\n",
      "Validation Results || Elbo loss: 106.88975807472512 || KL loss: 0.5859791735808054 || NLLL 106.30030073942962 || Perp: 69.26689447277452 ||MU loss 7.478139559427897\n",
      "New Best Validation score of 106.88975807472512!\n",
      "Iteration: 3100 || NLLL: 95.17172241210938 || Perp: 43.344383157856015 || KL Loss: 0.33720090985298157 || MuLoss: 0.6471381187438965 || Total: 96.15606689453125\n",
      "Done training the VAE\n",
      "Finished training for free_bits_param:0-mu_force_beta_param:5-param_wdropout_k:1!!!\n"
     ]
    }
   ],
   "source": [
    "from trainers.trainer_vae import train_vae\n",
    "\n",
    "for param_setting in param_grid:\n",
    "\n",
    "    # Copy config, set new variables\n",
    "    run_config = deepcopy(config)\n",
    "    run_config.freebits_param = param_setting['free_bits_param']\n",
    "    run_config.mu_force_beta_param = param_setting['mu_force_beta_param']\n",
    "    run_config.param_wdropout_k = param_setting['param_wdropout_k']\n",
    "\n",
    "    vae = make_vae(run_config).to(run_config.device)\n",
    "    optimizer = torch.optim.Adam(params=vae.parameters())\n",
    "\n",
    "    # Format to be a compact string representation\n",
    "    path_to_results = f'{run_config.results_path}/vae'\n",
    "    params2string = '-'.join([f\"{i}:{param_setting[i]}\" for i in param_setting.keys()])\n",
    "\n",
    "    results_writer = ResultsWriter(\n",
    "        label=f'{run_config.run_label}--vae-{params2string}',\n",
    "    )\n",
    "\n",
    "    sentence_decoder = utils.make_sentence_decoder(cd.tokenizer, 1)\n",
    "\n",
    "    if run_config.will_train_vae:\n",
    "        print(f\"Training params: {params2string}\")\n",
    "        train_vae(\n",
    "            vae,\n",
    "            optimizer,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            nr_epochs=run_config.nr_epochs,\n",
    "            device=run_config.device,\n",
    "            results_writer=results_writer,\n",
    "            config=run_config,\n",
    "            decoder=sentence_decoder,\n",
    "        )\n",
    "\n",
    "    results_writer.tensorboard_writer.close()\n",
    "\n",
    "    print(f\"Finished training for {params2string}!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: Analysis - Best model selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the decision was made to only vary the hyperparameters of the counter posterior collapse methods, and to keep the other models fixed as close as possible to the parameters Bowman et al. chose in their paper. \n",
    "\n",
    "Based on the param-grid defined above, we can define our based models by any potential metrics. One such metric that can be chosen is to get the models which minimize perplexity. As such, to do this, we use __tensorboard__ to explore the various trajectories of the model's metrics (such as perplexity, KL, etc.). Tensorboard can't be embedded into notebooks at the moment of writing, as far as we know, so instead CSVs are stored with validation performances of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "validation_csv_paths = list(Path('results/runs').glob('**/valid.csv'))\n",
    "validation_df = pd.concat([pd.read_csv(path) for path in validation_csv_paths])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting dataframe, `validation_df`, all results of the runs so far are stored. It is also possible to query for the specific run_label by editing the glob search string, though this might not always be desired as multiple runs could provide a larger scope of which models are interesting.\n",
    "\n",
    "Using this dataframe, we can naively select the best models by grabbing the mean across all runs. The intuition here is that as this experiment is performed more often, models with the same parameters will have more measurements, and start to approximate their actual performance better (and become invariant from coincidence of evaluation noise). However, due to the scope of this notebook, we run this experiment only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_performances = validation_df.groupby('label').mean().sort_values('perp_metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best VAE, we make the simple assumption currently to select the model with the highest perplexity.\n",
    "This can similarily be done for other selections, such as a model with only free-bits / only mu-force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vae_parameter = mean_performances.iloc[0]\n",
    "\n",
    "best_muforce_only_parameter = mean_performances.query('freebits_param == 0 and word_dropout == 1').iloc[0]\n",
    "best_freebits_only_parameter = mean_performances.query('mu_force_beta_param == 0 and word_dropout == 1').iloc[0]\n",
    "\n",
    "print(f\"Best VAE has parameters: {best_vae_parameter.name} \\n\"\n",
    "     f\"Best VAE with only mu force has parameters: {best_muforce_only_parameter.name} \\n\"\n",
    "     f\"Best VAE with only freebits has parameters {best_freebits_only_parameter.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how these optimal params have higher KL's than the close to 0 vanilla. It seems like this is a stronger effect for only freebits though, contributing to the idea that freebits has a much stronger effect on the KL in our implementation than mu-force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_vae_parameter)\n",
    "print(best_muforce_only_parameter)\n",
    "print(best_freebits_only_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard based\n",
    "In case you have tensorboard open, you can examine all models trained. We based our decision off the lowest models in perplexity, corresponding almost always with the highest model in KL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Evaluating these selected models on our test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we have our selection of models, all we need to do now is to load them in, load in our test dataset, and run them against our various models, measuring the perplexity meanwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a test data loader. Shuffle = False, Batch Size = 64\n",
      "Length of test data: 38\n"
     ]
    }
   ],
   "source": [
    "# Predefined selected best models\n",
    "best_saved_models = [\n",
    "    'vae_best_mu0_wd1_fb0.pt', # Vanilla VAE\n",
    "    'vae_best_mu2_wd1_fb2.pt', # Mu only\n",
    "    'vae_best_mu0_wd1_fb2.pt', # Freebits only\n",
    "    'vae_best_mu5_wd1_fb0.pt', # Best in total: Freebits and mu\n",
    "]\n",
    "\n",
    "test_loader = cd.get_data_loader(type='test', shuffle=False)\n",
    "print(f\"Length of test data: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model vae_best_mu2_wd1_fb2.pt: \n",
      "\n",
      "Test Results || Elbo loss: 125.95756892154091 || KL loss: 24.57783538416812 || NLLL 101.3797331358257 || Perp: 56.05236310996025 ||MU loss 61.16006439610531\n",
      "For model vae_best_mu0_wd1_fb2.pt: \n",
      "\n",
      "Test Results || Elbo loss: 125.70883118478875 || KL loss: 23.484046333714534 || NLLL 102.22478505184776 || Perp: 58.11954359587994 ||MU loss 70.91709869786312\n",
      "For model vae_best_mu5_wd1_fb0.pt: \n",
      "\n",
      "Test Results || Elbo loss: 107.74100183185779 || KL loss: 0.5465875628747439 || NLLL 107.19441433956749 || Perp: 70.62220585998166 ||MU loss 6.876160006774099\n"
     ]
    }
   ],
   "source": [
    "from metrics import make_elbo_criterion\n",
    "from models.make_vae import make_vae\n",
    "from inference.evaluate_vae import evaluate_vae\n",
    "\n",
    "for model_path in best_saved_models:\n",
    "    if 'vae' in model_path:\n",
    "        vae = make_vae(\n",
    "            config, \n",
    "            trained=True, \n",
    "            model_path=f'{config.results_path}/saved_models/{model_path}'\n",
    "        ).to(config.device)\n",
    "\n",
    "        loss_fn = make_elbo_criterion(config.vocab_size, config.vae_latent_size, -1, 0)\n",
    "\n",
    "        (test_total_loss, test_total_kl_loss, test_total_nlll, test_total_mu_loss), test_perp = evaluate_vae(\n",
    "            vae,\n",
    "            test_loader,\n",
    "            -1,\n",
    "            config.device,\n",
    "            loss_fn,\n",
    "            0,\n",
    "            'test'\n",
    "        )\n",
    "\n",
    "        print(f'For model {model_path}: \\n')\n",
    "        print(f'Test Results || Elbo loss: {test_total_loss} || KL loss: {test_total_kl_loss} || NLLL {test_total_nlll} || Perp: {test_perp} ||MU loss {test_total_mu_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the test is over, the models can't be finetuned more. For further intuition into these results, the paper provides a general overview. To summarize: Freebits seems to improve performance the most, while driving the KL up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Analysis: Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given these models, the next check we can do is to see how well they perform on text generation. For this, the idea is to give all these models a particular sentence, and see how well they do in continuing the latter part. This sampling is done in a greedy-fashion.\n",
    "\n",
    "As a start, a simple sentence is chosen: 'The chairman proposed that next fall a new treaty __is signed__'. The bolded words are to be predicted, but anything goes as long as it makes sense. Each time this is run, the models will try new predictions on the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of the sentence: the chairman proposed that next fall a new treaty || Max Length 10 .\n",
      "[1, 9060, 1937, 7154, 9058, 6192, 3743, 585, 6177, 9286]\n",
      "The model from vae_best_mu0_wd1_fb0.pt predicts the following: \n",
      "\t the chairman proposed that next fall a new treaty , for the french electronics , loan rows of \n",
      "\n",
      "Start of the sentence: the chairman proposed that next fall a new treaty || Max Length 10 .\n",
      "[1, 9060, 1937, 7154, 9058, 6192, 3743, 585, 6177, 9286]\n",
      "The model from vae_best_mu2_wd1_fb2.pt predicts the following: \n",
      "\t the chairman proposed that next fall a new treaty has headed to take advantage of permission in maturing \n",
      "\n",
      "Start of the sentence: the chairman proposed that next fall a new treaty || Max Length 10 .\n",
      "[1, 9060, 1937, 7154, 9058, 6192, 3743, 585, 6177, 9286]\n",
      "The model from vae_best_mu0_wd1_fb2.pt predicts the following: \n",
      "\t the chairman proposed that next fall a new treaty in that case , moody 's inc. of the trust \n",
      "\n",
      "Start of the sentence: the chairman proposed that next fall a new treaty || Max Length 10 .\n",
      "[1, 9060, 1937, 7154, 9058, 6192, 3743, 585, 6177, 9286]\n",
      "The model from vae_best_mu5_wd1_fb0.pt predicts the following: \n",
      "\t the chairman proposed that next fall a new treaty for the white house is planning the president to take \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inference.text_generation import generate_next_words\n",
    "from models.make_vae import make_vae\n",
    "\n",
    "best_saved_models = [\n",
    "    'vae_best_mu0_wd1_fb0.pt', # Vanilla VAE\n",
    "    'vae_best_mu2_wd1_fb2.pt', # Mu only\n",
    "    'vae_best_mu0_wd1_fb2.pt', # Freebits only\n",
    "    'vae_best_mu5_wd1_fb0.pt', # Best in total: Freebits and mu\n",
    "]\n",
    "\n",
    "sample_sentence = 'the chairman proposed that next fall a new treaty'\n",
    "\n",
    "for model_path in best_saved_models:\n",
    "    vae = make_vae(\n",
    "        config, \n",
    "        trained=True, \n",
    "        model_path=f'{config.results_path}/saved_models/{model_path}'\n",
    "    ).to(config.device)\n",
    "    \n",
    "    # Set to only returns predictions, not posterior\n",
    "    vae.graph_mode = True\n",
    "    \n",
    "    sent = generate_next_words(vae, cd, sample_sentence, device=config.device)\n",
    "    decoded_sent = cd.tokenizer.decode(sent)\n",
    "    print(f'The model from {model_path} predicts the following: \\n'\n",
    "         f'\\t {cd.tokenizer.decode(sent)} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the quality of the generated sentences vary above, the difference between a vanilla VAE and that of the 'better' models (with higher KL values) is noticeable; the vanilla VAE in general has no specific knowledge about what the sentence is really about ('the chairman proposed that next fall a new treaty , for the french electronics , loan rows of'), where the other models start to make sense. Our 'best' model for instance 'the chairman proposed that next fall a new treaty drove margin ex-dividend over a year or next month', indicating a semantic topic for formal agreements over a date-range, whereas the model with the most freebits indicates monetary and policy 'that next fall a new treaty increase against $ 6.25 a pound and that no policy '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
