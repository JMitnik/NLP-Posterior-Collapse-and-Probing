{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python38264bitnlp2proj1condaec5a9df1fc714f369a993c7d07146e76","display_name":"Python 3.8.2 64-bit ('nlp2-proj1': conda)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6_I2HW_30ZM2","colab_type":"text"},"source":["## Initialization"]},{"cell_type":"markdown","metadata":{"id":"mjCGoO8F0Sjm","colab_type":"text"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"Yc4zj6a-w8uv","colab_type":"code","colab":{}},"source":["train_path = '/dgm_for_text_data/02-21.10way.clean'\n","valid_path = '/dgm_for_text_data/22.auto.clean'\n","test_path  = '/dgm_for_text_data/23.auto.clean'"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1FdkJONGTLg","colab_type":"text"},"source":["Our data files consist of lines of the Penn Tree Bank data set. Each line is a sentence in a tree shape. Let's pretty print the first line from the training set to see what we're dealing with!"]},{"cell_type":"code","metadata":{"id":"yA6zTgvX23Nb","colab_type":"code","colab":{}},"source":["import os\n","from nltk import Tree\n","from nltk.treeprettyprinter import TreePrettyPrinter\n","\n","def filereader(path):\n","    \"\"\"\n","        Read a PTS data file line by line\n","    \"\"\"\n","    with open(os.getcwd() + path, mode='r') as f:\n","        for line in f:\n","            yield line"],"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def convert_to_sentence(line):\n","    \"\"\"\n","        Converts a line of a PTS data file into a lower case sentence string\n","    \"\"\"\n","    tree = Tree.fromstring(line)\n","    sentence = ' '.join(tree.leaves()).lower()\n","    return sentence"]},{"cell_type":"code","metadata":{"id":"S6SBba3JN5zp","colab_type":"code","outputId":"acf1b250-8133-4ad0-cef7-8df874de6bbb","executionInfo":{"status":"ok","timestamp":1585837328604,"user_tz":-120,"elapsed":578,"user":{"displayName":"Pieter de Marez Oyens","photoUrl":"","userId":"00177755685194850706"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["line = next(filereader(train_path))\n","tree = Tree.fromstring(line)\n","print(line)\n","print(TreePrettyPrinter(tree))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"(TOP (S (PP (IN In) (NP (NP (DT an) (NNP Oct.) (CD 19) (NN review)) (PP (IN of) (NP (`` ``) (NP (DT The) (NN Misanthrope)) ('' '') (PP (IN at) (NP (NP (NNP Chicago) (POS 's)) (NNP Goodman) (NNP Theatre))))) (PRN (-LRB- -LRB-) (`` ``) (S (NP (JJ Revitalized) (NNS Classics)) (VP (VBP Take) (NP (DT the) (NNP Stage)) (PP (IN in) (NP (NNP Windy) (NNP City))))) (, ,) ('' '') (NP (NNP Leisure) (CC &) (NNP Arts)) (-RRB- -RRB-)))) (, ,) (NP (NP (NP (DT the) (NN role)) (PP (IN of) (NP (NNP Celimene)))) (, ,) (VP (VBN played) (PP (IN by) (NP (NNP Kim) (NNP Cattrall)))) (, ,)) (VP (VBD was) (VP (ADVP (RB mistakenly)) (VBN attributed) (PP (TO to) (NP (NNP Christina) (NNP Haag))))) (. .)))\n\n                                                                                                                                                                TOP                                                                                                                                                                   \n                                                                                                                                                                 |                                                                                                                                                                     \n                                                                                                                                                                 S                                                                                                                                                                    \n                                  _______________________________________________________________________________________________________________________________|__________________________________________________________________________________________________________________________________________________________________   \n                                 PP                                                                                                                                                                  |                                     |                                                         |                              | \n  _______________________________|____________________________________________________________                                                                                                       |                                     |                                                         |                              |  \n |                                                                                            NP                                                                                                     |                                     |                                                         |                              | \n |        ____________________________________________________________________________________|____________________________________________________                                                  |                                     |                                                         |                              |  \n |       |                                           PP                                                                                           PRN                                                |                                     |                                                         |                              | \n |       |                ___________________________|___                                      ____________________________________________________|____________________________________________     |                                     |                                                         |                              |  \n |       |               |                               NP                                   |    |                                 S                                |   |           |         |    |                                     NP                                                        VP                             | \n |       |               |    ___________________________|_____                               |    |                _________________|________                        |   |           |         |    |                 ____________________|___________________________________     _________________|_______                       |  \n |       |               |   |       |               |         PP                             |    |               |                          VP                      |   |           |         |    |                NP                   |              VP                   |   |                         VP                     | \n |       |               |   |       |               |    _____|_________                     |    |               |             _____________|_________              |   |           |         |    |        ________|_______             |     _________|___                 |   |       __________________|______                |  \n |       |               |   |       |               |   |               NP                   |    |               |            |        |              PP            |   |           |         |    |       |                PP           |    |             PP               |   |      |          |              PP              | \n |       |               |   |       |               |   |            ___|_____________       |    |               |            |        |          ____|____         |   |           |         |    |       |             ___|_____       |    |      _______|___             |   |      |          |        ______|______         |  \n |       NP              |   |       NP              |   |           NP        |       |      |    |               NP           |        NP        |         NP       |   |           NP        |    |       NP           |         NP     |    |     |           NP           |   |     ADVP        |       |             NP       | \n |    ___|_________      |   |    ___|_______        |   |      _____|___      |       |      |    |        _______|_____       |     ___|____     |     ____|___     |   |      _____|___      |    |    ___|___         |         |      |    |     |        ___|_____       |   |      |          |       |       ______|___     |  \n IN  DT NNP   CD   NN    IN  ``  DT          NN      ''  IN   NNP       POS   NNP     NNP   -LRB-  ``      JJ           NNS    VBP   DT      NNP   IN  NNP      NNP   ,   ''   NNP    CC NNP  -RRB-  ,   DT      NN       IN       NNP     ,   VBN    IN     NNP       NNP     ,  VBD     RB        VBN      TO    NNP        NNP   . \n |   |   |    |    |     |   |   |           |       |   |     |         |     |       |      |    |       |             |      |    |        |    |    |        |    |   |     |     |   |     |    |   |       |        |         |      |    |     |       |         |      |   |      |          |       |      |          |    |  \n In  an Oct.  19 review  of  `` The     Misanthrope  ''  at Chicago      's Goodman Theatre -LRB-  `` Revitalized     Classics Take the     Stage  in Windy     City  ,   '' Leisure  &  Arts -RRB-  ,  the     role      of     Celimene  ,  played  by     Kim     Cattrall  ,  was mistakenly attributed  to Christina     Haag  . \n\n"}]},{"cell_type":"markdown","metadata":{"id":"Ln1OYWtWNFyN","colab_type":"text"},"source":["We are, for now, just intereseted in the sentences, which is just the leaves of our tree\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we make our usable data sets"]},{"cell_type":"code","metadata":{"id":"jRH_blOMUttk","colab_type":"code","colab":{}},"source":["train_sents = [convert_to_sentence(l) for l in filereader(train_path)]\n","valid_sents = [convert_to_sentence(l) for l in filereader(valid_path)]\n","test_sents = [convert_to_sentence(l) for l in filereader(test_path)]"],"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"original: in an oct. 19 review of `` the misanthrope '' at chicago 's goodman theatre -lrb- `` revitalized classics take the stage in windy city , '' leisure & arts -rrb- , the role of celimene , played by kim cattrall , was mistakenly attributed to christina haag .\ntokenized:  [1, 4754, 926, 6325, 185, 7745, 6332, 584, 9060, 3, 10, 1161, 2002, 16, 4273, 9063, 22, 584, 3, 3, 8917, 9060, 8556, 4754, 3, 2067, 18, 10, 5363, 8, 1113, 24, 18, 9060, 7826, 6332, 3, 18, 6822, 1744, 5193, 3, 18, 9725, 5944, 1200, 9167, 3, 3, 25, 2]\ndecoded: [BOS] in an oct. 19 review of `` the [UNK] '' at chicago 's goodman theatre -lrb- `` [UNK] [UNK] take the stage in [UNK] city , '' leisure & arts -rrb- , the role of [UNK] , played by kim [UNK] , was mistakenly attributed to [UNK] [UNK] . [EOS]\n\noriginal: ms. haag plays elianti .\ntokenized:  [1, 6042, 3, 6826, 3, 25, 2]\ndecoded: [BOS] ms. [UNK] plays [UNK] . [EOS]\n\noriginal: rolls-royce motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 .\ntokenized:  [1, 3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167, 7554, 8603, 1161, 609, 41, 1860, 4754, 226, 25, 2]\ndecoded: [BOS] [UNK] motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 . [EOS]\n\noriginal: the luxury auto maker last year sold 1,214 cars in the u.s.\ntokenized:  [1, 9060, 5569, 1222, 5620, 5281, 9963, 8400, 3, 1860, 4754, 9060, 9388, 2]\ndecoded: [BOS] the luxury auto maker last year sold [UNK] cars in the u.s. [EOS]\n\noriginal: howard mosher , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets .\ntokenized:  [1, 4638, 3, 18, 6996, 936, 2005, 3617, 6341, 18, 7923, 4466, 977, 4348, 3991, 9060, 5569, 1222, 5620, 4754, 1640, 936, 3551, 18, 936, 4754, 3760, 3297, 5687, 25, 2]\ndecoded: [BOS] howard [UNK] , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets . [EOS]\n\n"}],"source":["from tokenizers import WordTokenizer\n","\n","# Train your tokenizer. Credits to the NLP2 team for creating this tokenizer\n","tokenizer = WordTokenizer(train_sents, max_vocab_size=10000)\n","\n","for sentence in train_sents[:5]:\n","    tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n","    sentence_decoded = tokenizer.decode(tokenized, skip_special_tokens=False) \n","\n","    print('original: ' + sentence)\n","    print('tokenized: ', tokenized)\n","    print('decoded: ' + sentence_decoded)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["Now create our own custom data sets in pytorch"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class PTBDataset(Dataset):\n","    \"\"\"\n","        A PTB Dataset\n","    \"\"\"\n","    def __init__(self, sentences, tokenizer):\n","        self.sentences = sentences\n","        self.tokenizer = tokenizer\n","    \n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","        item = self.sentences[idx]\n","        # We make sure to add the special tokens, since our models need to know how to start and end a sentence\n","        tokenized = self.tokenizer.encode(item, add_special_tokens=True)\n","        return tokenized"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"train/validation/test :: 39832/1700/2416\n"}],"source":["train_set = PTBDataset(train_sents, tokenizer)\n","valid_set = PTBDataset(valid_sents, tokenizer)\n","test_set = PTBDataset(test_sents, tokenizer)\n","\n","# Lets print some information about our datasets\n","print(f'train/validation/test :: {len(train_set)}/{len(valid_set)}/{len(test_set)}')"]},{"cell_type":"markdown","metadata":{},"source":["Now let's create dataloaders that can load/shuffle and batch our data. Since we will want to give our models all equel sized input i.e. sentences of equal length. * This may change *"]},{"cell_type":"code","execution_count":39,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[   1, 4754,  926, 6325,  185, 7745, 6332,  584, 9060,    3,   10, 1161,\n         2002,   16, 4273, 9063,   22,  584,    3,    3, 8917, 9060, 8556, 4754,\n            3, 2067,   18,   10, 5363,    8, 1113,   24,   18, 9060, 7826, 6332,\n            3,   18, 6822, 1744, 5193,    3,   18, 9725, 5944, 1200, 9167,    3,\n            3,   25,    2],\n        [   1, 6042,    3, 6826,    3,   25,    2,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0]])\n[[1, 4754, 926, 6325, 185, 7745, 6332, 584, 9060, 3, 10, 1161, 2002, 16, 4273, 9063, 22, 584, 3, 3, 8917, 9060, 8556, 4754, 3, 2067, 18, 10, 5363, 8, 1113, 24, 18, 9060, 7826, 6332, 3, 18, 6822, 1744, 5193, 3, 18, 9725, 5944, 1200, 9167, 3, 3, 25, 2], [1, 6042, 3, 6826, 3, 25, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\ntensor([[   1,    3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167,\n         7554, 8603, 1161,  609,   41, 1860, 4754,  226,   25,    2],\n        [   1, 9060, 5569, 1222, 5620, 5281, 9963, 8400,    3, 1860, 4754, 9060,\n         9388,    2,    0,    0,    0,    0,    0,    0,    0,    0]])\n[[1, 3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167, 7554, 8603, 1161, 609, 41, 1860, 4754, 226, 25, 2], [1, 9060, 5569, 1222, 5620, 5281, 9963, 8400, 3, 1860, 4754, 9060, 9388, 2, 0, 0, 0, 0, 0, 0, 0, 0]]\n\n"}],"source":["from torch.utils.data import DataLoader\n","import torch\n","\n","def padded_collate(batch):\n","    \"\"\"\n","     Pad each sentence to the length of the longest sentence in the batch\n","    \"\"\"\n","    sentence_lengths = [len(s) for s in batch]\n","    max_length = max(sentence_lengths)\n","    padded_batch = [s + [0] * (max_length - len(s)) for s in batch]\n","    return torch.LongTensor(padded_batch)\n","\n","train_loader = DataLoader(train_set, batch_size=2, shuffle=False, collate_fn=padded_collate)\n","\n","# Small test for a data loader\n","t = 0\n","for d in train_loader:\n","    print(d)\n","    print(d.tolist())\n","    print()\n","    t += 1\n","    if t == 2:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}