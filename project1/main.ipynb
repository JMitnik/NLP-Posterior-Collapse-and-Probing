{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python38264bitnlp2proj1condaec5a9df1fc714f369a993c7d07146e76","display_name":"Python 3.8.2 64-bit ('nlp2-proj1': conda)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6_I2HW_30ZM2","colab_type":"text"},"source":["## Initialization"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjCGoO8F0Sjm","colab_type":"text"},"source":["# Data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yc4zj6a-w8uv","colab_type":"code","colab":{}},"source":["train_path = '/dgm_for_text_data/02-21.10way.clean'\n","valid_path = '/dgm_for_text_data/22.auto.clean'\n","test_path  = '/dgm_for_text_data/23.auto.clean'"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1FdkJONGTLg","colab_type":"text"},"source":["Our data files consist of lines of the Penn Tree Bank data set. Each line is a sentence in a tree shape. Let's pretty print the first line from the training set to see what we're dealing with!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yA6zTgvX23Nb","colab_type":"code","colab":{}},"source":["import os\n","from nltk import Tree\n","from nltk.treeprettyprinter import TreePrettyPrinter\n","\n","def filereader(path:str):\n","    \"\"\"\n","        Opens a PTS datafile yields one line at a time.\n","    \"\"\"\n","    with open(os.getcwd() + path, mode='r') as f:\n","        for line in f:\n","            yield line"],"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def convert_to_sentence(line: str):\n","    \"\"\"\n","        Takes in a line from a PTS datafile and returns it as a lower-case string.\n","    \"\"\"\n","    tree = Tree.fromstring(line)\n","    sentence = ' '.join(tree.leaves()).lower()\n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["#### Let's see how our data looks:"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6SBba3JN5zp","colab_type":"code","outputId":"acf1b250-8133-4ad0-cef7-8df874de6bbb","executionInfo":{"status":"ok","timestamp":1585837328604,"user_tz":-120,"elapsed":578,"user":{"displayName":"Pieter de Marez Oyens","photoUrl":"","userId":"00177755685194850706"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["line = next(filereader(train_path))\n","print(f'Original: {line}')\n","print(f'Prased: {convert_to_sentence(line)}')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Original: (TOP (S (PP (IN In) (NP (NP (DT an) (NNP Oct.) (CD 19) (NN review)) (PP (IN of) (NP (`` ``) (NP (DT The) (NN Misanthrope)) ('' '') (PP (IN at) (NP (NP (NNP Chicago) (POS 's)) (NNP Goodman) (NNP Theatre))))) (PRN (-LRB- -LRB-) (`` ``) (S (NP (JJ Revitalized) (NNS Classics)) (VP (VBP Take) (NP (DT the) (NNP Stage)) (PP (IN in) (NP (NNP Windy) (NNP City))))) (, ,) ('' '') (NP (NNP Leisure) (CC &) (NNP Arts)) (-RRB- -RRB-)))) (, ,) (NP (NP (NP (DT the) (NN role)) (PP (IN of) (NP (NNP Celimene)))) (, ,) (VP (VBN played) (PP (IN by) (NP (NNP Kim) (NNP Cattrall)))) (, ,)) (VP (VBD was) (VP (ADVP (RB mistakenly)) (VBN attributed) (PP (TO to) (NP (NNP Christina) (NNP Haag))))) (. .)))\n\nPrased: in an oct. 19 review of `` the misanthrope '' at chicago 's goodman theatre -lrb- `` revitalized classics take the stage in windy city , '' leisure & arts -rrb- , the role of celimene , played by kim cattrall , was mistakenly attributed to christina haag .\n"}]},{"cell_type":"markdown","metadata":{},"source":["#### Creating our datasets\n","\n","We have the data, now we want to create dataloaders so we can shuffle and batch our data. For this we will use pytorch's built in classes."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRH_blOMUttk","colab_type":"code","colab":{}},"source":["# We have a training, validation and test data set. For each, we need a list of sentences.\n","train_sents = [convert_to_sentence(l) for l in filereader(train_path)]\n","valid_sents = [convert_to_sentence(l) for l in filereader(valid_path)]\n","test_sents = [convert_to_sentence(l) for l in filereader(test_path)]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["For our models, we tensors that are easily interpretable for our machines. For this, we convert our sentences to tensors where each word is represented by a number, also we want to have special character tokens and limit our vocabulary so our parameter space is a bit more managable. To do all this, we use the *tokenizers.py* file with the WordTokenizer class, presented by the NLP2-Team.\n","\n","*Note*: We had special tokens to our sentences such as BOS, EOS and UNK. The BOS and EOS tokens are important, because it tells the model what constitutes as the beginning and the end of a sentence.\n","\n","We see that, in the code block below, that add_special_tokens is set to True, and appends a BOS and EOS token to the sentences, and are represented as number 1 and 2 in tensor-form. We decode the sentences taking in these special tokens into account."],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"original: in an oct. 19 review of `` the misanthrope '' at chicago 's goodman theatre -lrb- `` revitalized classics take the stage in windy city , '' leisure & arts -rrb- , the role of celimene , played by kim cattrall , was mistakenly attributed to christina haag .\n----------\ntokenized:  [1, 4754, 926, 6325, 185, 7745, 6332, 584, 9060, 3, 10, 1161, 2002, 16, 4273, 9063, 22, 584, 3, 3, 8917, 9060, 8556, 4754, 3, 2067, 18, 10, 5363, 8, 1113, 24, 18, 9060, 7826, 6332, 3, 18, 6822, 1744, 5193, 3, 18, 9725, 5944, 1200, 9167, 3, 3, 25, 2]\n----------\ndecoded: [BOS] in an oct. 19 review of `` the [UNK] '' at chicago 's goodman theatre -lrb- `` [UNK] [UNK] take the stage in [UNK] city , '' leisure & arts -rrb- , the role of [UNK] , played by kim [UNK] , was mistakenly attributed to [UNK] [UNK] . [EOS]\n----------\n\n\n\noriginal: ms. haag plays elianti .\n----------\ntokenized:  [1, 6042, 3, 6826, 3, 25, 2]\n----------\ndecoded: [BOS] ms. [UNK] plays [UNK] . [EOS]\n----------\n\n\n\noriginal: rolls-royce motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 .\n----------\ntokenized:  [1, 3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167, 7554, 8603, 1161, 609, 41, 1860, 4754, 226, 25, 2]\n----------\ndecoded: [BOS] [UNK] motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 . [EOS]\n----------\n\n\n\noriginal: the luxury auto maker last year sold 1,214 cars in the u.s.\n----------\ntokenized:  [1, 9060, 5569, 1222, 5620, 5281, 9963, 8400, 3, 1860, 4754, 9060, 9388, 2]\n----------\ndecoded: [BOS] the luxury auto maker last year sold [UNK] cars in the u.s. [EOS]\n----------\n\n\n\noriginal: howard mosher , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets .\n----------\ntokenized:  [1, 4638, 3, 18, 6996, 936, 2005, 3617, 6341, 18, 7923, 4466, 977, 4348, 3991, 9060, 5569, 1222, 5620, 4754, 1640, 936, 3551, 18, 936, 4754, 3760, 3297, 5687, 25, 2]\n----------\ndecoded: [BOS] howard [UNK] , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets . [EOS]\n----------\n\n\n\n"}],"source":["from tokenizers import WordTokenizer\n","\n","# How big we want our vocabulary to be\n","vocab_size = 10000\n","\n","# Creating and train our tokenizer. We want a relatively small vocabulary of 10000 words. Credits to the NLP2 team for creating this tokenizer.\n","tokenizer = WordTokenizer(train_sents, max_vocab_size=vocab_size)\n","\n","# We check if the tokenizer en- and decodes our sentences correctly. Just look at the top-5 sentences in our training set.\n","for sentence in train_sents[:5]:\n","    tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n","    sentence_decoded = tokenizer.decode(tokenized, skip_special_tokens=False) \n","\n","    print('original: ' + sentence)\n","    print(f'{\"-\"*10}')\n","    print('tokenized: ', tokenized)\n","    print(f'{\"-\"*10}')\n","    print('decoded: ' + sentence_decoded)\n","    print(f'{\"-\"*10}')\n","    print('\\n\\n')"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating custom pytorch data sets\n","\n","To work with pytorch data loaders, we want custom pytorch dataset."],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class PTBDataset(Dataset):\n","    \"\"\"\n","        A custom PTB dataset. \n","    \"\"\"\n","    def __init__(self, sentences: list, tokenizer: WordTokenizer):\n","        self.sentences = sentences\n","        self.tokenizer = tokenizer\n","    \n","    def __len__(self):\n","        \"\"\"\n","            Return the length of the dataset.\n","        \"\"\"\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx: int):\n","        \"\"\"\n","            Returns a tokenized item at position idx from the dataset.\n","        \"\"\"\n","        item = self.sentences[idx]\n","        tokenized = self.tokenizer.encode(item, add_special_tokens=True)\n","        return tokenized"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"train/validation/test :: 39832/1700/2416\n"}],"source":["# We instantiate the datasets.\n","train_set = PTBDataset(train_sents, tokenizer)\n","valid_set = PTBDataset(valid_sents, tokenizer)\n","test_set = PTBDataset(test_sents, tokenizer)\n","\n","# Lets print some information about our datasets\n","print(f'train/validation/test :: {len(train_set)}/{len(valid_set)}/{len(test_set)}')"]},{"cell_type":"markdown","metadata":{},"source":["Now let's create dataloaders that can load/shuffle and batch our data. W\n","When pytorch batches our data, it stacks the tensors. However, since our sentences are not equal in size, their corresponding tensors will also have different sizes. To fix this problem, we pad each sentence in a batch to the size, taking our longest sentence as the target size. This way, stacking tensors won't be a problem."],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"tags":["outputPrepend"]},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import torch\n","\n","def padded_collate(batch: list):\n","    \"\"\"\n","     Pad each sentence to the length of the longest sentence in the batch\n","    \"\"\"\n","    sentence_lengths = [len(s) for s in batch]\n","    max_length = max(sentence_lengths)\n","    padded_batch = [s + [0] * (max_length - len(s)) for s in batch]\n","    return torch.LongTensor(padded_batch)"]},{"cell_type":"markdown","metadata":{},"source":["We want to test if our data loader works, so we create one of our test set with a tiny batch size of 2. From to batches, we print the output."],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[   1, 4754,  926, 6325,  185, 7745, 6332,  584, 9060,    3,   10, 1161,\n         2002,   16, 4273, 9063,   22,  584,    3,    3, 8917, 9060, 8556, 4754,\n            3, 2067,   18,   10, 5363,    8, 1113,   24,   18, 9060, 7826, 6332,\n            3,   18, 6822, 1744, 5193,    3,   18, 9725, 5944, 1200, 9167,    3,\n            3,   25,    2],\n        [   1, 6042,    3, 6826,    3,   25,    2,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0]])\n[[1, 4754, 926, 6325, 185, 7745, 6332, 584, 9060, 3, 10, 1161, 2002, 16, 4273, 9063, 22, 584, 3, 3, 8917, 9060, 8556, 4754, 3, 2067, 18, 10, 5363, 8, 1113, 24, 18, 9060, 7826, 6332, 3, 18, 6822, 1744, 5193, 3, 18, 9725, 5944, 1200, 9167, 3, 3, 25, 2], [1, 6042, 3, 6826, 3, 25, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n--------------------\ntensor([[   1,    3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167,\n         7554, 8603, 1161,  609,   41, 1860, 4754,  226,   25,    2],\n        [   1, 9060, 5569, 1222, 5620, 5281, 9963, 8400,    3, 1860, 4754, 9060,\n         9388,    2,    0,    0,    0,    0,    0,    0,    0,    0]])\n[[1, 3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167, 7554, 8603, 1161, 609, 41, 1860, 4754, 226, 25, 2], [1, 9060, 5569, 1222, 5620, 5281, 9963, 8400, 3, 1860, 4754, 9060, 9388, 2, 0, 0, 0, 0, 0, 0, 0, 0]]\n--------------------\n"}],"source":["train_loader = DataLoader(train_set, batch_size=2, shuffle=False, collate_fn=padded_collate)\n","\n","# Small test for a data loader\n","for di, d in enumerate(train_loader):\n","    print(d)\n","    print(d.tolist())\n","    print(f'{\"-\"*20}')\n","    if di == 1:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the Model\n","We import the model from our models folder. For encoding, this will be our RNNLM, defined in RNNLM.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from models.RNNLM import RNNLM\n","embedding_size = 50\n","hidden_size = 50\n","# vocab size is our vocab size, embedding is 500, hidden is 100. These number are arbitrary for now. Still trying to make the model work\n","rnnlm = RNNLM(vocab_size, embedding_size, hidden_size)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Define loss function\n","criterion = nn.NLLLoss()\n","optim = torch.optim.Adam(rnnlm.parameters())"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["all_losses = []\n","\n","loss = 0\n","# %%\n","def train_model_on_batch(model: RNNLM, optim: torch.optim.Optimizer, input_tensor: torch.Tensor):\n","    optim.zero_grad()\n","    # inp is to be shaped as Sentences(=batch) x Words\n","    hidden = model.init_hidden(input_tensor)\n","\n","    for idx in range(input_tensor.shape[1] - 1):\n","        current_words = input_tensor[:, idx] # Get the current word for each sentence\n","        print(current_words)\n","        print(f'{\"-\"*20}')\n","        output, hidden = model(current_words, hidden)\n","        print(output)\n","        print(output.size())\n","        break\n","        # next_words = inp[:, idx + 1]\n","\n","        # # TODO: Ensure this works\n","        # local_loss = criterion(torch.log(pred), torch.tensor(next_words))\n","\n","        # loss += local_loss\n","\n","    # loss.backward()\n","    # optim.step()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[   1, 4754,  926, 6325,  185, 7745, 6332,  584, 9060,    3,   10, 1161,\n         2002,   16, 4273, 9063,   22,  584,    3,    3, 8917, 9060, 8556, 4754,\n            3, 2067,   18,   10, 5363,    8, 1113,   24,   18, 9060, 7826, 6332,\n            3,   18, 6822, 1744, 5193,    3,   18, 9725, 5944, 1200, 9167,    3,\n            3,   25,    2],\n        [   1, 6042,    3, 6826,    3,   25,    2,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0]])\n--------------------\ntensor([1, 1])\n--------------------\ntensor([[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n       grad_fn=<SoftmaxBackward>)\ntorch.Size([2, 10000])\n"}],"source":["for batch in train_loader:\n","    print(batch)\n","    print(f'{\"-\"*20}')\n","    train_model_on_batch(rnnlm, optim, batch)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}