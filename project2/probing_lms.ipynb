{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probing Language Models__\n",
    "\n",
    "This notebook serves as a start for your NLP2 assignment on probing Language Models. This notebook will become part of the contents that you will submit at the end, so make sure to keep your code (somewhat) clean :-)\n",
    "\n",
    "__note__: This is the first time _anyone_ is doing this assignment. That's exciting! But it might well be the case that certain aspects are too unclear. Do not hesitate at all to reach to me once you get stuck, I'd be grateful to help you out.\n",
    "\n",
    "__note 2__: This assignment is not dependent on big fancy GPUs. I run all this stuff on my own 3 year old CPU, without any Colab hassle. So it's up to you to decide how you want to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the Transformer models you are advised to make use of the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "Their library is well documented, and they provide great tools to easily load in pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Configuration|\n",
    "from config import Config\n",
    "\n",
    "config: Config = Config(\n",
    "    run_label='full_data',\n",
    "    uses_sample=False,\n",
    "    path_to_data_train='data/en_ewt-ud-train.conllu',\n",
    "    path_to_data_valid='data/en_ewt-ud-dev.conllu',\n",
    "    feature_model_type='LSTM',\n",
    "    will_train_simple_probe=True,\n",
    "    will_control_task_simple_prob=False,\n",
    "    will_train_structural_probe=False,\n",
    "    will_train_dependency_probe=True,\n",
    "    will_control_task_dependency_probe=True,\n",
    "    struct_probe_train_epoch=100,\n",
    "    dep_probe_train_epoch=1000,\n",
    "    struct_probe_lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load our feature models\n",
    "from models.model_inits import make_pretrained_lstm_and_tokenizer\n",
    "\n",
    "# The Gulordava LSTM model can be found here:\n",
    "# https://drive.google.com/open?id=1w47WsZcZzPyBKDn83cMNd0Hb336e-_Sy\n",
    "\n",
    "# Initializing the LSTM\n",
    "lstm, lstm_vocab = make_pretrained_lstm_and_tokenizer()\n",
    "\n",
    "# Initializing the Transformer\n",
    "trans_model_type: str = config.feature_model_type if config.feature_model_type is not 'LSTM' else config.default_trans_model_type\n",
    "trans_model = GPT2Model.from_pretrained(trans_model_type)\n",
    "trans_tokenizer = GPT2Tokenizer.from_pretrained(trans_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this assignment you will train your probes on __treebank__ corpora. A treebank is a corpus that has been *parsed*, and stored in a representation that allows the parse tree to be recovered. Next to a parse tree, treebanks also often contain information about part-of-speech tags, which is exactly what we are after now.\n",
    "\n",
    "The treebank you will use for now is part of the Universal Dependencies project. I provide a sample of this treebank as well, so you can test your setup on that before moving on to larger amounts of data.\n",
    "\n",
    "Make sure you accustom yourself to the format that is created by the `conllu` library that parses the treebank files before moving on. For example, make sure you understand how you can access the pos tag of a token, or how to cope with the tree structure that is formed using the `to_tree()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Union\n",
    "from conllu import parse_incr, TokenList\n",
    "from data_tools.data_inits import parse_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÅ\n",
    "# Some utility data-elements for reference\n",
    "sample_corpus: List[TokenList] = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "sample_sents: List[TokenList] = sample_corpus[:1]\n",
    "sample_sent: TokenList = sample_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÅ\n",
    "# Utility functions for dealing with the conllu dataset\n",
    "from typing import Callable\n",
    "\n",
    "get_pos_from_sent: Callable[[TokenList], List[str]] = lambda sent: [word['upostag'] for word in sent]\n",
    "get_tokens_from_sent: Callable[[TokenList], List[str]] = lambda sent: [word['form'] for word in sent]\n",
    "get_ids_from_sent: Callable[[TokenList], List[str]] = lambda sent: [word['id'] for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Representations\n",
    "\n",
    "We now have our data all set, our models are running and we are good to go!\n",
    "\n",
    "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
    "\n",
    "There are a few things you should keep in mind here. Read these carefully, as these tips will save you a lot of time in your implementation.\n",
    "- Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "- Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. This is great for our setup! Otherwise it would have been quite challenging to distribute the representation of a subword over the 2 tokens it belongs to.\n",
    "\n",
    "- If you closely examine the provided treebank, you will notice that some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. The tokenizer for GPT-2 adds spaces at the start of a token.\n",
    "\n",
    "- The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. Make sure you keep the above points in mind though, when creating the LSTM representations. You might want to write separate functions for the LSTM, but that is up to you.\n",
    "\n",
    "I would like to stress that if you feel hindered in any way by the simple code structure that is presented here, you are free to modify it :-) Just make sure it is clear to an outsider what you're doing, some helpful comments never hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed basic checks!\n"
     ]
    }
   ],
   "source": [
    "# FETCH SENTENCE REPRESENTATIONS\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from data_tools.feature_extractors import fetch_sen_reps\n",
    "\n",
    "# A sanity check was provided by the supervisor of this project, to compare representations with a 'gold standard'.\n",
    "def assert_sen_reps(transformer_model, transformer_tokenizer, lstm_model, lstm_tokenizer):\n",
    "    with open('lstm_emb1.pickle', 'rb') as f:\n",
    "        lstm_emb1: torch.Tensor = pickle.load(f)\n",
    "    with open('distilgpt2_emb1.pickle', 'rb') as f:\n",
    "        distilgpt2_emb1: torch.Tensor = pickle.load(f)\n",
    "\n",
    "    corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "\n",
    "    own_gpt2_emb1: torch.Tensor = fetch_sen_reps(corpus, transformer_model, transformer_tokenizer)[0]\n",
    "    own_lstm_emb1: torch.Tensor = fetch_sen_reps(corpus, lstm_model, lstm_tokenizer)[0]\n",
    "\n",
    "    assert distilgpt2_emb1.shape == own_gpt2_emb1.shape, \"GPT2 Shapes don't match!\"\n",
    "    assert lstm_emb1.shape == own_lstm_emb1.shape, \"LSTM Shapes don't match!\"\n",
    "\n",
    "    assert torch.allclose(distilgpt2_emb1, own_gpt2_emb1, atol=1e-04), \"GPT2 Embeddings don't match!\"\n",
    "    assert torch.allclose(lstm_emb1, own_lstm_emb1, atol=1e-04), \"LSTM Embeddings don't match!\"\n",
    "\n",
    "    print('Passed basic checks!')\n",
    "\n",
    "assert_sen_reps(trans_model, trans_tokenizer, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# üèÅ\n",
    "from typing import Tuple\n",
    "\n",
    "def fetch_pos_tags(ud_parses: List[TokenList],\n",
    "                    pos_vocab: Optional[DefaultDict[str, int]] = None,\n",
    "                    corrupted=False,\n",
    "                    corrupted_pos_tags: Optional[Dict[str, str]] = None)-> Tuple[List[Tensor], DefaultDict]:\n",
    "    \"\"\"\n",
    "    Converts `ud_parses` into a tensor of POS tags.\n",
    "    \"\"\"\n",
    "    # If `pos_vocab` is not known, make one based on all POS tokens in `ud_parses`\n",
    "    if (pos_vocab is None):\n",
    "        print('get all tokens')\n",
    "        all_pos_tokens = set([pos for sent in ud_parses for pos in get_pos_from_sent(sent)])\n",
    "        print('get all pos2i')\n",
    "        pos2i = {'<pad>': 0, '<unk>': 1, **{pos.strip(): i + 2 for i, pos in enumerate(all_pos_tokens)}}\n",
    "        pos_vocab = defaultdict(lambda: pos2i[\"<unk>\"])\n",
    "        pos_vocab.update(pos2i)\n",
    "    pos_tokens_result: List[Tensor] = []\n",
    "\n",
    "    sent: TokenList\n",
    "\n",
    "    for sent in ud_parses:\n",
    "        # If corrupted, let the target value be the corrupted tokens\n",
    "        if corrupted:\n",
    "            pos_tokens = torch.tensor([pos_vocab[corrupted_pos_tags[word]] for word in get_tokens_from_sent(sent)])\n",
    "        else:\n",
    "            pos_tokens = torch.tensor([pos_vocab[pos] for pos in get_pos_from_sent(sent)])\n",
    "        pos_tokens_result.append(pos_tokens)\n",
    "\n",
    "\n",
    "    return pos_tokens_result, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important: We will use LSTM as our model representation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Here we decide which language model to use `lm`,\n",
    "if config.feature_model_type == 'LSTM':\n",
    "    print(\"Important: We will use LSTM as our model representation\")\n",
    "    model = lstm\n",
    "    model_name = 'LSTM'\n",
    "    config.feature_model_dimensionality = 650\n",
    "    w2i = vocab\n",
    "else:\n",
    "    print(f\"Important: We will use {config.feature_model_type} as our model representation\")\n",
    "    config.feature_model_dimensionality = 768\n",
    "    model = trans_model\n",
    "    w2i = trans_tokenizer\n",
    "\n",
    "use_sample = config.uses_sample\n",
    "\n",
    "# Function that combines the previous functions, and creates 2 tensors for a .conllu file:\n",
    "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
    "\n",
    "def create_data(\n",
    "    filename: str,\n",
    "    model,\n",
    "    w2i: Dict[str, int],\n",
    "    pos_vocab=None,\n",
    "    corrupted=False,\n",
    "    corrupted_pos_tags: Optional[Dict[str, str]] = None\n",
    "):\n",
    "    print('Parsing the corpus')\n",
    "    ud_parses = parse_corpus(filename)\n",
    "\n",
    "    print('Fetching the POS tags')\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses,\n",
    "                                            pos_vocab=pos_vocab,\n",
    "                                            corrupted=corrupted,\n",
    "                                            corrupted_pos_tags=corrupted_pos_tags)\n",
    "    print(f'Fetching sen reps using {type(model).__name__} in `create_data`')\n",
    "    sen_reps = fetch_sen_reps(ud_parses, model, w2i)\n",
    "\n",
    "\n",
    "    return sen_reps, pos_tags, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create Corrupted Data\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def create_corrupted_tokens(use_sample: bool) -> Dict[str, str]:\n",
    "    # Get sentences from all data sets\n",
    "    ud_parses = []\n",
    "    for set_type in ['train', 'dev', 'test']:\n",
    "        filename = os.path.join('data', 'sample' if use_sample else '', f'en_ewt-ud-{set_type}.conllu')\n",
    "\n",
    "        ud_parses += (parse_corpus(filename))\n",
    "\n",
    "    all_pos_tokens = list(set([pos for sent in ud_parses for pos in get_pos_from_sent(sent)]))\n",
    "    all_pos_tokens = ['<pad>', '<unk>', *all_pos_tokens]\n",
    "\n",
    "    possible_targets_distr = Categorical(torch.tensor([1/len(all_pos_tokens) for _ in range(len(all_pos_tokens))]))\n",
    "\n",
    "    corrupted_word_type: DefaultDict[str, str] = defaultdict(lambda: \"UNK\")\n",
    "    # Get a corrupted POS tag for each word\n",
    "    for sentence in ud_parses:\n",
    "        for token in sentence:\n",
    "\n",
    "            corrupted_pos_tag = all_pos_tokens[possible_targets_distr.sample()]\n",
    "\n",
    "            if token['form'] not in corrupted_word_type:\n",
    "                corrupted_word_type[token['form']] = corrupted_pos_tag\n",
    "\n",
    "    return corrupted_word_type\n",
    "\n",
    " # Get out corrupted pos tokens for each word type\n",
    "corrupted_pos_tags: Dict[str, str] = create_corrupted_tokens(use_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Utility functions for transforming X and y to appropriate formats\n",
    "def transform_XY_to_concat_tensors(X: List[Tensor], y: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    # X_concat = torch.cat([x.reshape(1) for x in X], dim=0)\n",
    "    X_concat = torch.cat(X, dim=0)\n",
    "    y_concat = torch.cat(y, dim=0)\n",
    "\n",
    "    return X_concat, y_concat\n",
    "\n",
    "def transform_XY_to_padded_tensors(X: List[Tensor], y: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    X_padded = pad_sequence(X)\n",
    "    y_padded = pad_sequence(y)\n",
    "\n",
    "    return X_padded, y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Classification\n",
    "\n",
    "We now have our models, our data, _and_ our representations all set! Hurray, well done. We can finally move onto the cool stuff, i.e. training the diagnostic classifiers (DCs).\n",
    "\n",
    "DCs are simple in their complexity on purpose. To read more about why this is the case you could already have a look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2).\n",
    "\n",
    "A simple linear classifier will suffice for now, don't bother with adding fancy non-linearities to it.\n",
    "\n",
    "I am personally a fan of the `skorch` library, that provides `sklearn`-like functionalities for training `torch` models, but you are free to train your dc using whatever method you prefer.\n",
    "\n",
    "As this is an Artificial Intelligence master and you have all done ML1 + DL, I expect you to use your train/dev/test splits correctly ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the corpus\n",
      "Fetching the POS tags\n",
      "get all tokens\n",
      "get all pos2i\n",
      "Fetching sen reps using RNNModel in `create_data`\n",
      "Parsing the corpus\n",
      "Fetching the POS tags\n",
      "Fetching sen reps using RNNModel in `create_data`\n",
      "Training POS probe\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.7539\u001b[0m        \u001b[32m1.2302\u001b[0m       \u001b[35m0.8186\u001b[0m        \u001b[31m0.7120\u001b[0m  5.1824\n",
      "      2       \u001b[36m0.8481\u001b[0m        \u001b[32m0.5730\u001b[0m       \u001b[35m0.8471\u001b[0m        \u001b[31m0.5386\u001b[0m  4.7088\n",
      "      3       \u001b[36m0.8691\u001b[0m        \u001b[32m0.4670\u001b[0m       \u001b[35m0.8605\u001b[0m        \u001b[31m0.4788\u001b[0m  5.0986\n",
      "      4       \u001b[36m0.8796\u001b[0m        \u001b[32m0.4208\u001b[0m       \u001b[35m0.8679\u001b[0m        \u001b[31m0.4479\u001b[0m  5.0005\n",
      "      5       \u001b[36m0.8860\u001b[0m        \u001b[32m0.3938\u001b[0m       \u001b[35m0.8720\u001b[0m        \u001b[31m0.4286\u001b[0m  4.9894\n",
      "      6       \u001b[36m0.8900\u001b[0m        \u001b[32m0.3756\u001b[0m       \u001b[35m0.8748\u001b[0m        \u001b[31m0.4151\u001b[0m  5.0663\n",
      "      7       \u001b[36m0.8927\u001b[0m        \u001b[32m0.3625\u001b[0m       \u001b[35m0.8766\u001b[0m        \u001b[31m0.4055\u001b[0m  4.9720\n",
      "      8       \u001b[36m0.8948\u001b[0m        \u001b[32m0.3524\u001b[0m       \u001b[35m0.8778\u001b[0m        \u001b[31m0.3979\u001b[0m  4.9013\n",
      "      9       \u001b[36m0.8968\u001b[0m        \u001b[32m0.3445\u001b[0m       \u001b[35m0.8790\u001b[0m        \u001b[31m0.3920\u001b[0m  4.3231\n",
      "     10       \u001b[36m0.8982\u001b[0m        \u001b[32m0.3379\u001b[0m       \u001b[35m0.8807\u001b[0m        \u001b[31m0.3871\u001b[0m  4.6662\n",
      "     11       \u001b[36m0.8997\u001b[0m        \u001b[32m0.3325\u001b[0m       \u001b[35m0.8819\u001b[0m        \u001b[31m0.3831\u001b[0m  5.0421\n",
      "     12       \u001b[36m0.9008\u001b[0m        \u001b[32m0.3279\u001b[0m       \u001b[35m0.8829\u001b[0m        \u001b[31m0.3798\u001b[0m  5.0675\n",
      "     13       \u001b[36m0.9017\u001b[0m        \u001b[32m0.3239\u001b[0m       \u001b[35m0.8834\u001b[0m        \u001b[31m0.3771\u001b[0m  5.0405\n",
      "     14       \u001b[36m0.9026\u001b[0m        \u001b[32m0.3204\u001b[0m       \u001b[35m0.8835\u001b[0m        \u001b[31m0.3745\u001b[0m  5.0884\n",
      "     15       \u001b[36m0.9034\u001b[0m        \u001b[32m0.3173\u001b[0m       \u001b[35m0.8846\u001b[0m        \u001b[31m0.3723\u001b[0m  4.9869\n",
      "     16       \u001b[36m0.9038\u001b[0m        \u001b[32m0.3146\u001b[0m       \u001b[35m0.8853\u001b[0m        \u001b[31m0.3704\u001b[0m  5.0292\n",
      "     17       \u001b[36m0.9044\u001b[0m        \u001b[32m0.3122\u001b[0m       \u001b[35m0.8856\u001b[0m        \u001b[31m0.3689\u001b[0m  5.0643\n",
      "     18       \u001b[36m0.9049\u001b[0m        \u001b[32m0.3099\u001b[0m       \u001b[35m0.8858\u001b[0m        \u001b[31m0.3675\u001b[0m  5.2539\n",
      "     19       \u001b[36m0.9054\u001b[0m        \u001b[32m0.3079\u001b[0m       \u001b[35m0.8860\u001b[0m        \u001b[31m0.3661\u001b[0m  4.9624\n",
      "     20       \u001b[36m0.9058\u001b[0m        \u001b[32m0.3061\u001b[0m       \u001b[35m0.8864\u001b[0m        \u001b[31m0.3649\u001b[0m  4.3333\n",
      "     21       \u001b[36m0.9060\u001b[0m        \u001b[32m0.3044\u001b[0m       0.8864        \u001b[31m0.3640\u001b[0m  4.3468\n",
      "     22       \u001b[36m0.9063\u001b[0m        \u001b[32m0.3028\u001b[0m       \u001b[35m0.8869\u001b[0m        \u001b[31m0.3628\u001b[0m  4.3335\n",
      "     23       \u001b[36m0.9066\u001b[0m        \u001b[32m0.3014\u001b[0m       \u001b[35m0.8875\u001b[0m        \u001b[31m0.3624\u001b[0m  4.3097\n",
      "     24       \u001b[36m0.9069\u001b[0m        \u001b[32m0.3000\u001b[0m       \u001b[35m0.8875\u001b[0m        \u001b[31m0.3613\u001b[0m  5.0065\n",
      "     25       \u001b[36m0.9072\u001b[0m        \u001b[32m0.2988\u001b[0m       \u001b[35m0.8877\u001b[0m        \u001b[31m0.3608\u001b[0m  5.4809\n",
      "     26       \u001b[36m0.9074\u001b[0m        \u001b[32m0.2976\u001b[0m       \u001b[35m0.8878\u001b[0m        \u001b[31m0.3598\u001b[0m  4.5610\n",
      "     27       \u001b[36m0.9079\u001b[0m        \u001b[32m0.2965\u001b[0m       0.8877        \u001b[31m0.3594\u001b[0m  4.3347\n",
      "     28       \u001b[36m0.9080\u001b[0m        \u001b[32m0.2955\u001b[0m       \u001b[35m0.8880\u001b[0m        \u001b[31m0.3588\u001b[0m  4.7889\n",
      "     29       \u001b[36m0.9083\u001b[0m        \u001b[32m0.2945\u001b[0m       \u001b[35m0.8881\u001b[0m        \u001b[31m0.3585\u001b[0m  5.3201\n",
      "     30       \u001b[36m0.9085\u001b[0m        \u001b[32m0.2936\u001b[0m       0.8879        \u001b[31m0.3578\u001b[0m  4.4629\n",
      "     31       \u001b[36m0.9086\u001b[0m        \u001b[32m0.2927\u001b[0m       0.8879        \u001b[31m0.3575\u001b[0m  4.3425\n",
      "Stopping since valid_acc has not improved in the last 4 epochs.\n",
      "Done Training Probe\n",
      "Validation Accuracy: 0.8878\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "# üèÅ\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from models.probes import SimpleProbe\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring, Checkpoint, TrainEndCheckpoint, LoadInitState, EarlyStopping\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.history import History\n",
    "\n",
    "from tools.results_writer import ResultsWriter\n",
    "\n",
    "rw = ResultsWriter(config)\n",
    "\n",
    "# Helper function to calculate accuracy\n",
    "accuracy = lambda preds, targets: sum([1 if pred == target else 0 for pred, target in zip(preds, targets)]) / len(preds)\n",
    "pos_probe_results = defaultdict(list)\n",
    "valid_acc = None\n",
    "train_acc = EpochScoring(scoring='accuracy',\n",
    "                        on_train=True,\n",
    "                        name='train_acc',\n",
    "                        lower_is_better=False)\n",
    "early_stopping = EarlyStopping(monitor='valid_acc',\n",
    "                                patience=config.pos_probe_train_patience,\n",
    "                                lower_is_better=False)\n",
    "callbacks = [train_acc, early_stopping]\n",
    "\n",
    "if config.will_train_simple_probe:\n",
    "    train_data_X, train_data_y, train_vocab = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "        model,\n",
    "        w2i\n",
    "    )\n",
    "\n",
    "    valid_data_X, valid_data_y, _ = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "        model,\n",
    "        w2i,\n",
    "        pos_vocab=train_vocab\n",
    "    )\n",
    "\n",
    "    train_X, train_y = transform_XY_to_concat_tensors(train_data_X, train_data_y)\n",
    "    valid_X, valid_y = transform_XY_to_concat_tensors(valid_data_X, valid_data_y)\n",
    "    valid_ds = Dataset(valid_X, valid_y)\n",
    "\n",
    "    print('Training POS probe')\n",
    "    train_acc = EpochScoring(scoring='accuracy',\n",
    "                            on_train=True,\n",
    "                            name='train_acc',\n",
    "                            lower_is_better=False)\n",
    "    early_stopping = EarlyStopping(monitor='valid_acc',\n",
    "                                    patience=config.pos_probe_train_patience,\n",
    "                                    lower_is_better=False)\n",
    "    callbacks = [train_acc, early_stopping]\n",
    "\n",
    "    embedding_size = train_data_X[0].shape[1] # size of the word embedding (either 650 or 768)\n",
    "    vocab_size = len(train_vocab)#17\n",
    "\n",
    "    probe: nn.Module = SimpleProbe(\n",
    "        embedding_size,\n",
    "        vocab_size\n",
    "    )\n",
    "\n",
    "    net: NeuralNetClassifier = NeuralNetClassifier(\n",
    "        probe,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=config.pos_probe_train_epoch,\n",
    "        batch_size=config.pos_probe_train_batch_size,\n",
    "        lr=config.pos_probe_train_lr,\n",
    "        train_split=predefined_split(valid_ds),\n",
    "        iterator_train__shuffle=True,\n",
    "        optimizer= torch.optim.Adam,\n",
    "    )\n",
    "\n",
    "    net.fit(train_X, train_y)\n",
    "    print('Done Training Probe')\n",
    "\n",
    "    # Get accuracy score\n",
    "    valid_predictions = net.predict(valid_X)\n",
    "    valid_acc = accuracy(valid_predictions, valid_y)\n",
    "    print(f'Validation Accuracy: {valid_acc:.4f}')\n",
    "\n",
    "    # Get validation losses / accuracies from skorch's history\n",
    "    net_history = net.history\n",
    "    validation_losses = net_history[:,'valid_loss']\n",
    "    validation_accs = net_history[:, 'valid_acc']\n",
    "    pos_probe_results['validation_losses'] = validation_losses\n",
    "    pos_probe_results['validation_accs'] = validation_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "title": "Train a corrupted prob classifier"
   },
   "outputs": [],
   "source": [
    "if config.will_control_task_simple_prob:\n",
    "    corrupted_train_data_X, corrupted_train_data_y, corrupted_train_vocab = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "        model,\n",
    "        w2i,\n",
    "        corrupted=True,\n",
    "        corrupted_pos_tags=corrupted_pos_tags\n",
    "    )\n",
    "\n",
    "    corrupted_valid_data_X, corrupted_valid_data_y, _ = create_data(\n",
    "        os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "        model,\n",
    "        w2i,\n",
    "        pos_vocab=corrupted_train_vocab,\n",
    "        corrupted=True,\n",
    "        corrupted_pos_tags=corrupted_pos_tags\n",
    "    )\n",
    "\n",
    "    embedding_size = corrupted_train_data_X[0].shape[1] # size of the word embedding (either 650 or 768)\n",
    "    vocab_size = len(corrupted_train_vocab)#17\n",
    "\n",
    "    probe: nn.Module = SimpleProbe(\n",
    "        embedding_size,\n",
    "        vocab_size\n",
    "    )\n",
    "\n",
    "    corrupted_train_X, corrupted_train_y = transform_XY_to_concat_tensors(corrupted_train_data_X, corrupted_train_data_y)\n",
    "    corrupted_valid_X, corrupted_valid_y = transform_XY_to_concat_tensors(corrupted_valid_data_X, corrupted_valid_data_y)\n",
    "    corrupted_valid_ds = Dataset(corrupted_valid_X, corrupted_valid_y)\n",
    "\n",
    "    corrupted_probe: nn.Module = SimpleProbe(\n",
    "        embedding_size,\n",
    "        vocab_size\n",
    "    )\n",
    "\n",
    "    corrupted_net: NeuralNetClassifier = NeuralNetClassifier(\n",
    "        probe,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=config.pos_probe_train_epoch,\n",
    "        batch_size=config.pos_probe_train_batch_size,\n",
    "        lr=config.pos_probe_train_lr,\n",
    "        train_split=predefined_split(corrupted_valid_ds),\n",
    "        iterator_train__shuffle=True,\n",
    "        optimizer= torch.optim.Adam,\n",
    "    )\n",
    "\n",
    "    corrupted_net.fit(corrupted_train_X, corrupted_train_y)\n",
    "\n",
    "    corrupted_valid_preds = corrupted_net.predict(corrupted_valid_X)\n",
    "    corrupted_valid_acc = accuracy(corrupted_valid_preds, corrupted_valid_y)\n",
    "    print(f'Corrupted Validation Accuracy: {corrupted_valid_acc:.4f}')\n",
    "    corrupted_total_epochs = len(corrupted_net.history)\n",
    "\n",
    "    if valid_acc is not None:\n",
    "        valid_selectivity = valid_acc - corrupted_valid_acc\n",
    "        print(f'Validation Selectivity: {valid_selectivity:.4f}')\n",
    "\n",
    "    corrupted_history = corrupted_net.history\n",
    "    corrupted_validation_losses = corrupted_history[:,'valid_loss']\n",
    "    corrupted_validation_accs = corrupted_history[:,'valid_acc']\n",
    "    pos_probe_results['corrupted_validation_losses'] = corrupted_validation_losses\n",
    "    pos_probe_results['corrupted_validation_accs'] = corrupted_validation_accs\n",
    "\n",
    "# Now write results at the end of the POS\n",
    "if config.will_train_simple_probe or config.will_control_task_simple_prob:\n",
    "    rw.write_results('POS', 'Transformer', config.feature_model_type, results=pos_probe_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Visualize data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably cant read file Transformer.csv\n",
      "Probably cant read the LSTM.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    trans_result_df = pd.read_csv('results/POS_probe/Transformer.csv')\n",
    "    trans_result_df = trans_result_df.rename(columns={'validation_accs':'Trans Val Accs', 'c_validation_accs':'Corr Trans Val Accs'})\n",
    "    ax1 = sns.lineplot(data=trans_result_df[['Trans Val Accs', 'Corr Trans Val Accs']], alpha=0.7)\n",
    "except:\n",
    "    print(\"Probably cant read file Transformer.csv\")\n",
    "\n",
    "try:\n",
    "    lstm_result_df = pd.read_csv('results/POS_probe/LSTM.csv')\n",
    "    lstm_result_df = lstm_result_df.rename(columns={'validation_accs':'LSTM Val Accs', 'c_validation_accs':'Corr LSTM Val Accs'})\n",
    "    ax2 = sns.lineplot(data=lstm_result_df[['LSTM Val Accs', 'Corr LSTM Val Accs']], alpha=0.7)\n",
    "except:\n",
    "    print(\"Probably cant read the LSTM.csv\")\n",
    "\n",
    "sns.set_palette(sns.color_palette(\"BuGn_r\"))\n",
    "sns.set_palette(sns.light_palette(\"navy\", reverse=True))\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_probe_validation_accs(filename: str, model_type: str, column_names:dict):\n",
    "    assert model_type in ['POS', 'Edge', 'Structural'], 'model type needs to be: POS, Edge or Structural.'\n",
    "    assert 'val_accs_column' in column_names, 'Missing key(val_accs_column):value(...)'\n",
    "    assert 'corrupted_val_accs_column' in column_names, 'Missing key(corrupted_val_accs_column):value(...)'\n",
    "\n",
    "    transformer_df = pd.read_csv(f'results/{model_type}/trans_{filename}.csv')\n",
    "    transformer_df = transformer_df.rename(columns={column_names['val_accs_column']:'Trans Val Accs', [column_names[1]]:'Corr Trans Val Accs'})\n",
    "\n",
    "    lstm_df = pd.read_csv(f'results/{model_type}/lstm_{filename}.csv')\n",
    "    lstm_df = lstm_df.rename(columns={column_names['corrupted_val_accs_column']:'LSTM Val Accs', [column_names[1]]:'LSTM Trans Val Accs'})\n",
    "\n",
    "    sns.set_palette(sns.color_palette(\"BuGn_r\"))\n",
    "    ax1 = sns.lineplot(data=trans_result_df[['Trans Val Accs', 'Corr Trans Val Accs']], alpha=0.7)\n",
    "    sns.set_palette(sns.light_palette(\"navy\", reverse=True))\n",
    "    ax2 = sns.lineplot(data=lstm_result_df[['LSTM Val Accs', 'Corr LSTM Val Accs']], alpha=0.7)\n",
    "\n",
    "\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "\n",
    "def plot_probe_validation_accs_by_modelfiles(filename_LSTM: str, filename_Transformer, probe_type: str):\n",
    "    assert probe_type in ['POS', 'dep_edge_probe', 'struct_probe'], 'model type needs to be: POS, Edge or Structural.'\n",
    "    transformer_df = pd.read_csv(filename_Transformer)\n",
    "    lstm_df = pd.read_csv(filename_LSTM)\n",
    "\n",
    "    sns.set_palette(sns.color_palette(\"BuGn_r\"))\n",
    "    # ax1 = sns.lineplot(data=transfo[['Trans Val Accs', 'Corr Trans Val Accs']], alpha=0.7)\n",
    "\n",
    "    sns.set_palette(sns.light_palette(\"navy\", reverse=True))\n",
    "    ax2 = sns.lineplot(data=lstm_result_df[['LSTM Val Accs', 'Corr LSTM Val Accs']], alpha=0.7)\n",
    "\n",
    "\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly. I have provided code that transforms a `TokenTree` to a `Tree` in `ete3` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tools.tree_tools import tokentree_to_ete, tokentree_to_nltk, edges, create_mst\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
    "\n",
    "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances\n",
    "\n",
    "We will store all these distances in a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
    "\n",
    "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). Fortunately for us, we can simply import a method from `scipy` that retrieves this MST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus.\n",
    "\n",
    "If your addition to the `create_gold_distances` method has been correct, you should be able to run the following snippet. This then shows you the original parse tree, the distances between the nodes, and the MST that is retrieved from these distances. Can you spot the edges in the MST matrix that correspond to the edges in the parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   /2 /-1\n",
      "  |\n",
      "  |--3\n",
      "  |\n",
      "  |--4\n",
      "  |\n",
      "  |   /-6\n",
      "  |  |\n",
      "-5|  |--7\n",
      "  |-8|\n",
      "  |  |   /-9\n",
      "  |  |  |\n",
      "  |   \\12--10\n",
      "  |     |\n",
      "  |      \\-11\n",
      "  |\n",
      "   \\-13 \n",
      "\n",
      "tensor([[0., 1., 3., 3., 2., 4., 4., 3., 5., 5., 5., 4., 3.],\n",
      "        [1., 0., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
      "        [3., 2., 0., 2., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
      "        [3., 2., 2., 0., 1., 3., 3., 2., 4., 4., 4., 3., 2.],\n",
      "        [2., 1., 1., 1., 0., 2., 2., 1., 3., 3., 3., 2., 1.],\n",
      "        [4., 3., 3., 3., 2., 0., 2., 1., 3., 3., 3., 2., 3.],\n",
      "        [4., 3., 3., 3., 2., 2., 0., 1., 3., 3., 3., 2., 3.],\n",
      "        [3., 2., 2., 2., 1., 1., 1., 0., 2., 2., 2., 1., 2.],\n",
      "        [5., 4., 4., 4., 3., 3., 3., 2., 0., 2., 2., 1., 4.],\n",
      "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 0., 2., 1., 4.],\n",
      "        [5., 4., 4., 4., 3., 3., 3., 2., 2., 2., 0., 1., 4.],\n",
      "        [4., 3., 3., 3., 2., 2., 2., 1., 1., 1., 1., 0., 3.],\n",
      "        [3., 2., 2., 2., 1., 3., 3., 2., 4., 4., 4., 3., 0.]]) \n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Read corpus,\n",
    "corpus = parse_corpus(config.path_to_data_train)\n",
    "sample_sent = corpus[5]\n",
    "\n",
    "# Convert sentence to tree\n",
    "sample_tokentree = sample_sent.to_tree()\n",
    "sample_ete3_tree = tokentree_to_ete(sample_tokentree)\n",
    "print(ete3_tree, '\\n')\n",
    "\n",
    "# Extract gold-distance from sentence\n",
    "sample_gold_distance = create_struct_gold_distances([sample_sent])[0]\n",
    "print(gold_distance, '\\n')\n",
    "\n",
    "# Transform gold-distance to minimum spanning tree\n",
    "sample_mst = create_mst(sample_gold_distance)\n",
    "print(sample_mst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
    "\n",
    "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
    "\n",
    "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n",
    "\n",
    "You will write code that computes the UUAS score for a matrix of predicted distances, and the corresponding gold distances. I recommend you to split this up into 2 methods: 1 that retrieves the edges that are present in an MST matrix, and one general method that computes the UUAS score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Probes\n",
    "\n",
    "We will now train the structural probe. The Probing model used is founded on John Hewitt's source code.\n",
    "\n",
    "For this task, we will make use of the `train_struct` function as main training loop, located in `runners/trainers`. Furthermore, the `make_struct_dataloaders` will initialize the dataloaders with the features again as X, and correct edge distances as `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from data_tools.datasets import ProbingDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners.trainers import train_struct\n",
    "from models.model_inits import make_pretrained_lstm_and_tokenizer, make_pretrained_transformer_and_tokenizer\n",
    "from data_tools.dataloaders import make_struct_dataloaders\n",
    "\n",
    "# We will train the structural probe if `config.will_train_structural_probe` allows it.\n",
    "if config.will_train_structural_probe:\n",
    "    print(\"Loading in data for structural probe!\")\n",
    "    train_dataloader, valid_dataloader = make_struct_dataloaders(\n",
    "        path_to_train=config.path_to_data_train,\n",
    "        path_to_valid=config.path_to_data_valid,\n",
    "        feature_model=model,\n",
    "        feature_model_tokenizer=w2i,\n",
    "    )\n",
    "\n",
    "    probe, losses, uuas = train_struct(\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        nr_epochs=config.struct_probe_train_epoch,\n",
    "        struct_emb_dim=embedding_size,\n",
    "        struct_lr=config.struct_probe_lr,\n",
    "        struct_rank=config.struct_probe_rank,\n",
    "    )\n",
    "\n",
    "    struct_probe_results = {\n",
    "        'probe_valid_losses': losses,\n",
    "        'probe_valid_uuas_scores': uuas\n",
    "    }\n",
    "\n",
    "    rw.write_results('struct', config.feature_model_type, '', struct_probe_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "# Dependency Probing Parent Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the experimental setup, a different will be considered. To prevent any wrong conclusions based on ill-conceived ideas of control tasks for control tasks, a new task will be considered for this specific purpose. Based on John Hewitt and Percy Liang's work on 'Designing and Interpreting Probes with Control Tasks', this next task will consider a different the task of finding a node's parent, instead of finding all edges.\n",
    "\n",
    "This will be treated like a classificiation task, where given N potential nodes, the goal is to predict which of the N nodes is the parent, with Negative Log Likelihood as main loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets for Dependency parsing\n",
      "Starting training for Dependency parsing\n",
      "\n",
      "---- EPOCH 1 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 2.606781244277954\n",
      "Average evaluation accuracy score is 0.3054361939430237\n",
      "New loss has been reached of 2.606781244277954 being lower than lowest_loss inf\n",
      "\n",
      "---- EPOCH 2 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 2.155669927597046\n",
      "Average evaluation accuracy score is 0.31256192922592163\n",
      "New loss has been reached of 2.155669927597046 being lower than lowest_loss 2.606781244277954\n",
      "\n",
      "---- EPOCH 3 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.9864505529403687\n",
      "Average evaluation accuracy score is 0.351388156414032\n",
      "New loss has been reached of 1.9864505529403687 being lower than lowest_loss 2.155669927597046\n",
      "\n",
      "---- EPOCH 4 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.8776729106903076\n",
      "Average evaluation accuracy score is 0.3826623260974884\n",
      "New loss has been reached of 1.8776729106903076 being lower than lowest_loss 1.9864505529403687\n",
      "\n",
      "---- EPOCH 5 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.8081122636795044\n",
      "Average evaluation accuracy score is 0.39389151334762573\n",
      "New loss has been reached of 1.8081122636795044 being lower than lowest_loss 1.8776729106903076\n",
      "\n",
      "---- EPOCH 6 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.7581207752227783\n",
      "Average evaluation accuracy score is 0.4174753427505493\n",
      "New loss has been reached of 1.7581207752227783 being lower than lowest_loss 1.8081122636795044\n",
      "\n",
      "---- EPOCH 7 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.7197966575622559\n",
      "Average evaluation accuracy score is 0.42628252506256104\n",
      "New loss has been reached of 1.7197966575622559 being lower than lowest_loss 1.7581207752227783\n",
      "\n",
      "---- EPOCH 8 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6892536878585815\n",
      "Average evaluation accuracy score is 0.4336225688457489\n",
      "New loss has been reached of 1.6892536878585815 being lower than lowest_loss 1.7197966575622559\n",
      "\n",
      "---- EPOCH 9 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6704214811325073\n",
      "Average evaluation accuracy score is 0.43604573607444763\n",
      "New loss has been reached of 1.6704214811325073 being lower than lowest_loss 1.6892536878585815\n",
      "\n",
      "---- EPOCH 10 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6611226797103882\n",
      "Average evaluation accuracy score is 0.4404466450214386\n",
      "New loss has been reached of 1.6611226797103882 being lower than lowest_loss 1.6704214811325073\n",
      "\n",
      "---- EPOCH 11 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6413451433181763\n",
      "Average evaluation accuracy score is 0.45544785261154175\n",
      "New loss has been reached of 1.6413451433181763 being lower than lowest_loss 1.6611226797103882\n",
      "\n",
      "---- EPOCH 12 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6344293355941772\n",
      "Average evaluation accuracy score is 0.4680160582065582\n",
      "New loss has been reached of 1.6344293355941772 being lower than lowest_loss 1.6413451433181763\n",
      "\n",
      "---- EPOCH 13 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6337168216705322\n",
      "Average evaluation accuracy score is 0.46514639258384705\n",
      "New loss has been reached of 1.6337168216705322 being lower than lowest_loss 1.6344293355941772\n",
      "\n",
      "---- EPOCH 14 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6348880529403687\n",
      "Average evaluation accuracy score is 0.47367748618125916\n",
      "...are we overfitting? Setting patience_counter to 1\n",
      "\n",
      "---- EPOCH 15 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6300535202026367\n",
      "Average evaluation accuracy score is 0.47897839546203613\n",
      "New loss has been reached of 1.6300535202026367 being lower than lowest_loss 1.6337168216705322\n",
      "\n",
      "---- EPOCH 16 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6252816915512085\n",
      "Average evaluation accuracy score is 0.48699748516082764\n",
      "New loss has been reached of 1.6252816915512085 being lower than lowest_loss 1.6300535202026367\n",
      "\n",
      "---- EPOCH 17 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.631494164466858\n",
      "Average evaluation accuracy score is 0.48344963788986206\n",
      "...are we overfitting? Setting patience_counter to 1\n",
      "\n",
      "---- EPOCH 18 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6434693336486816\n",
      "Average evaluation accuracy score is 0.490954726934433\n",
      "...are we overfitting? Setting patience_counter to 2\n",
      "\n",
      "---- EPOCH 19 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6458475589752197\n",
      "Average evaluation accuracy score is 0.49323996901512146\n",
      "...are we overfitting? Setting patience_counter to 3\n",
      "\n",
      "---- EPOCH 20 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6436054706573486\n",
      "Average evaluation accuracy score is 0.4846036732196808\n",
      "...are we overfitting? Setting patience_counter to 4\n",
      "\n",
      "---- EPOCH 21 ---- \n",
      "\n",
      "Encountered: null sentence at idx 3\n",
      "Average evaluation loss score is 1.6419428586959839\n",
      "Average evaluation accuracy score is 0.48041021823883057\n",
      "Started to overfit, patience has been reached\n"
     ]
    }
   ],
   "source": [
    "# Initialize results for which we might want to write something\n",
    "dep_probe_results = defaultdict(list)\n",
    "\n",
    "if config.will_train_dependency_probe:\n",
    "    print(\"Loading datasets for Dependency parsing\")\n",
    "    train_dataloader, valid_dataloader = make_struct_dataloaders(\n",
    "        config.path_to_data_train,\n",
    "        config.path_to_data_valid,\n",
    "        feature_model=model,\n",
    "        feature_model_tokenizer=w2i,\n",
    "        use_dependencies=True\n",
    "    )\n",
    "\n",
    "    print(\"Starting training for Dependency parsing\")\n",
    "    dep_trained_probe, dep_valid_losses, dep_valid_acc = train_dep_parsing(\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        feature_dim=config.feature_model_dimensionality,\n",
    "        probe_rank=config.dep_probe_rank,\n",
    "        lr=config.dep_probe_lr,\n",
    "        nr_epochs=config.dep_probe_train_epoch\n",
    "    )\n",
    "\n",
    "    # We then store some results\n",
    "    dep_probe_results['valid_losses'] = dep_valid_losses\n",
    "    dep_probe_results['valid_acc'] = dep_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets for Dependency Probes - Control Task\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-54c7623edb95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0muse_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0muse_corrupted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrupted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_dep_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/uvadev/nlp2/project2/data_tools/dataloaders.py\u001b[0m in \u001b[0;36mmake_struct_dataloaders\u001b[0;34m(path_to_train, path_to_valid, feature_model, feature_model_tokenizer, train_batch_size, valid_batch_size, use_dependencies, use_corrupted, use_shuffled_dataset, corrupted_vocab, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0muse_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_dependencies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0muse_corrupted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_corrupted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdep_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProbingDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/uvadev/nlp2/project2/data_tools/data_inits.py\u001b[0m in \u001b[0;36minit_tree_corpus\u001b[0;34m(path, feature_model, feature_tokenizer, concat, cutoff, use_dependencies, use_corrupted, dep_vocab)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You should provide a vocabulary with these indices.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mgold_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dep_parent_gold_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_corrupted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/uvadev/nlp2/project2/data_tools/target_extractors.py\u001b[0m in \u001b[0;36mcreate_dep_parent_gold_distances\u001b[0;34m(corpus, corrupted, vocab)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mchild_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'form'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mcorrupted_choice_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrupted_choices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrupted_choice_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from data_tools.data_inits import parse_all_corpora\n",
    "from runners.trainers import train_dep_parsing\n",
    "\n",
    "# Control task time\n",
    "if config.will_control_task_dependency_probe:\n",
    "    # Read all corpora and make a vocab for how to deal with all possible tokens\n",
    "    all_corpora = parse_all_corpora(True)\n",
    "    corrupted_dep_vocab = create_corrupted_dep_vocab(all_corpora)\n",
    "\n",
    "    print(\"Loading datasets for Dependency Probes - Control Task\")\n",
    "    train_dataloader, valid_dataloader = make_struct_dataloaders(\n",
    "        config.path_to_data_train,\n",
    "        config.path_to_data_valid,\n",
    "        feature_model=model,\n",
    "        feature_model_tokenizer=w2i,\n",
    "        use_dependencies=True,\n",
    "        use_corrupted=True,\n",
    "        corrupted_vocab=corrupted_dep_vocab\n",
    "    )\n",
    "\n",
    "    print(\"Starting training for Dependency parsing - Control Task\")\n",
    "    dep_trained_probe, dep_valid_losses, dep_valid_acc = train_dep_parsing(\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        feature_dim=config.feature_model_dimensionality,\n",
    "        probe_rank=config.dep_probe_rank,\n",
    "        lr=config.dep_probe_lr,\n",
    "        nr_epochs=config.dep_probe_train_epoch,\n",
    "    )\n",
    "\n",
    "    dep_probe_results['corrupted_valid_losses'] = corrupted_dep_valid_losses\n",
    "    dep_probe_results['corrupted_dep_valid_acc'] = corrupted_dep_valid_acc\n",
    "\n",
    "if config.will_control_task_dependency_probe or config.will_train_dependency_probe:\n",
    "    rw.write_results('dep_edge', config.feature_model_type, '', dep_probe_results)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
